è¿™æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„é¡¹ç›®ï¼ä½¿ç”¨ KVM å’Œ Libvirt æ­å»º Hadoop é›†ç¾¤å¯ä»¥è®©æ‚¨æ·±å…¥äº†è§£è™šæ‹ŸåŒ–å’ŒHadoopé›†ç¾¤çš„åº•å±‚å·¥ä½œåŸç†ã€‚è¿™æ¯”ä½¿ç”¨ Docker æ›´æ¥è¿‘â€œè£¸é‡‘å±â€éƒ¨ç½²ã€‚

è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†å…¶åˆ†è§£ä¸ºå‡ ä¸ªå…³é”®é˜¶æ®µã€‚æˆ‘ä¼šå…ˆç»™æ‚¨ä¸€ä¸ªå®Œæ•´çš„è·¯çº¿å›¾ï¼Œç„¶åè¯¦ç»†è¯´æ˜ç¬¬ä¸€é˜¶æ®µçš„æ­¥éª¤ã€‚

### è·¯çº¿å›¾ï¼šKVM/Libvirt ä¸Šçš„ Hadoop é›†ç¾¤

1.  **é˜¶æ®µä¸€ï¼šå®¿ä¸»æœºå‡†å¤‡**

      * æ£€æŸ¥å’Œå¯ç”¨è™šæ‹ŸåŒ–æ”¯æŒã€‚
      * å®‰è£… KVMã€Libvirt å’Œå¿…è¦çš„ç®¡ç†å·¥å…·ï¼ˆå¦‚ `virt-manager`ï¼‰ã€‚
      * é…ç½®ç”¨æˆ·æƒé™å’Œ Libvirt æœåŠ¡ã€‚

2.  **é˜¶æ®µäºŒï¼šåˆ›å»ºâ€œé»„é‡‘é•œåƒâ€è™šæ‹Ÿæœº**

      * ä¸‹è½½ä¸€ä¸ªæœåŠ¡å™¨æ“ä½œç³»ç»Ÿï¼ˆæ¨è Ubuntu Server 20.04/22.04 æˆ– CentOS 7/Rocky Linuxï¼‰ã€‚
      * åˆ›å»ºç¬¬ä¸€ä¸ªè™šæ‹Ÿæœºï¼ˆVMï¼‰ä½œä¸ºæ¨¡æ¿ã€‚
      * åœ¨æ­¤VMä¸Šå®‰è£…é€šç”¨è½¯ä»¶ï¼šJava (Hadoop è¿è¡Œå¿…å¤‡)ã€`openssh-server` ç­‰ã€‚
      * åˆ›å»º `hadoop` ç”¨æˆ·ã€‚

3.  **é˜¶æ®µä¸‰ï¼šå…‹éš†è™šæ‹Ÿæœº**

      * ä½¿ç”¨â€œé»„é‡‘é•œåƒâ€å…‹éš†å‡ºé›†ç¾¤æ‰€éœ€çš„æ‰€æœ‰èŠ‚ç‚¹ï¼ˆä¾‹å¦‚ï¼š1ä¸ª `hadoop-namenode`ï¼Œ2ä¸ª `hadoop-datanode`ï¼‰ã€‚
      * ä¸ºæ¯ä¸ªå…‹éš†çš„VMé…ç½®**é™æ€IPåœ°å€**å’Œ**ä¸»æœºå**ã€‚
      * åœ¨æ‰€æœ‰VMä¸Šé…ç½® `/etc/hosts` æ–‡ä»¶ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥é€šè¿‡ä¸»æœºåç›¸äº’é€šä¿¡ã€‚

4.  **é˜¶æ®µå››ï¼šHadoop å®‰è£…ä¸é…ç½®**

      * åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šä¸‹è½½å¹¶è§£å‹ Hadoopã€‚
      * åœ¨ NameNode ä¸Šé…ç½®**æ— å¯†ç SSH**ï¼Œä½¿å…¶å¯ä»¥æ§åˆ¶æ‰€æœ‰ DataNodeã€‚
      * ç¼–è¾‘ Hadoop æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼ˆ`core-site.xml`, `hdfs-site.xml`, `mapred-site.xml`, `yarn-site.xml`, `workers`ï¼‰ã€‚

5.  **é˜¶æ®µäº”ï¼šå¯åŠ¨ä¸æµ‹è¯•**

      * åœ¨ NameNode ä¸Š**æ ¼å¼åŒ– HDFS**ã€‚
      * å¯åŠ¨ HDFS å’Œ YARN æœåŠ¡ã€‚
      * ä½¿ç”¨ `jps` å‘½ä»¤æ£€æŸ¥æ‰€æœ‰Javaè¿›ç¨‹æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚
      * è¿è¡Œä¸€ä¸ªç¤ºä¾‹ MapReduce ä»»åŠ¡ï¼ˆå¦‚ WordCountï¼‰æ¥éªŒè¯é›†ç¾¤ã€‚

-----

### é˜¶æ®µä¸€ï¼šå®¿ä¸»æœºå‡†å¤‡ï¼ˆè¯¦ç»†æ­¥éª¤ï¼‰

æˆ‘ä»¬é¦–å…ˆç¡®ä¿æ‚¨çš„ç‰©ç†æœºï¼ˆå®¿ä¸»æœºï¼‰å·²å‡†å¤‡å¥½è¿è¡Œ KVMã€‚æˆ‘å°†å‡è®¾æ‚¨çš„å®¿ä¸»æœºè¿è¡Œçš„æ˜¯åŸºäº Debian/Ubuntu çš„ Linux ç³»ç»Ÿã€‚

#### 1\. æ£€æŸ¥è™šæ‹ŸåŒ–æ”¯æŒ

æ‚¨çš„ CPU å¿…é¡»æ”¯æŒç¡¬ä»¶è™šæ‹ŸåŒ–ï¼ˆIntel VT-x æˆ– AMD-Vï¼‰ã€‚

```bash
# æ£€æŸ¥CPUæ˜¯å¦æ”¯æŒè™šæ‹ŸåŒ–ã€‚å¦‚æœè¾“å‡ºå¤§äº 0ï¼Œåˆ™è¡¨ç¤ºæ”¯æŒã€‚
egrep -c '(vmx|svm)' /proc/cpuinfo

# ç¡®ä¿ KVM æ¨¡å—å·²åŠ è½½
lsmod | grep kvm
```

å¦‚æœ `egrep` å‘½ä»¤è¾“å‡ºä¸º 0ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨ BIOS/UEFI ä¸­å¯ç”¨è™šæ‹ŸåŒ–ã€‚

#### 2\. å®‰è£… KVM å’Œ Libvirt

æˆ‘ä»¬å°†å®‰è£… KVMï¼ˆè™šæ‹Ÿæœºç›‘è§†å™¨ï¼‰ã€Libvirtï¼ˆç®¡ç†APIï¼‰å’Œ `virt-manager`ï¼ˆä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å›¾å½¢åŒ–ç®¡ç†å·¥å…·ï¼‰ã€‚

```bash
# æ›´æ–°è½¯ä»¶åŒ…åˆ—è¡¨
sudo apt update

# å®‰è£…æ‰€æœ‰å¿…è¦çš„è½¯ä»¶åŒ…
sudo apt install -y qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virt-manager
```

  * `qemu-kvm`: æ ¸å¿ƒçš„è™šæ‹Ÿæœºç›‘è§†å™¨ã€‚
  * `libvirt-daemon-system`: Libvirt æœåŠ¡ã€‚
  * `libvirt-clients`: åŒ…å« `virsh` ç­‰å‘½ä»¤è¡Œå·¥å…·ã€‚
  * `bridge-utils`: ç”¨äºåˆ›å»ºå’Œç®¡ç†ç½‘ç»œæ¡¥æ¥ï¼ˆå¯¹é«˜çº§ç½‘ç»œæœ‰ç”¨ï¼‰ã€‚
  * `virt-manager`: (å¯é€‰ä½†å¼ºçƒˆæ¨è) ä¸€ä¸ªGUIå·¥å…·ï¼Œç”¨äºåˆ›å»ºã€ç®¡ç†å’Œç›‘æ§VMã€‚

#### 3\. é…ç½®ç”¨æˆ·æƒé™

ä¸ºäº†è®©æ‚¨èƒ½å¤Ÿä»¥æ™®é€šç”¨æˆ·èº«ä»½ç®¡ç†è™šæ‹Ÿæœºï¼ˆè€Œä¸æ˜¯æ€»æ˜¯ä½¿ç”¨ `sudo`ï¼‰ï¼Œæ‚¨éœ€è¦å°†æ‚¨çš„ç”¨æˆ·æ·»åŠ åˆ° `libvirt` å’Œ `kvm` ç»„ã€‚

```bash
# å°†å½“å‰ç”¨æˆ·æ·»åŠ åˆ° libvirt å’Œ kvm ç»„
sudo adduser $(whoami) libvirt
sudo adduser $(whoami) kvm
```

> **é‡è¦æç¤º**ï¼šæ·»åŠ ç»„åï¼Œæ‚¨**å¿…é¡»å®Œå…¨æ³¨é”€å¹¶é‡æ–°ç™»å½•**ï¼ˆæˆ–é‡å¯ç”µè„‘ï¼‰ï¼Œæ‰èƒ½ä½¿ç»„æ›´æ”¹ç”Ÿæ•ˆã€‚

#### 4\. éªŒè¯å®‰è£…

é‡æ–°ç™»å½•åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥éªŒè¯ä¸€åˆ‡æ˜¯å¦æ­£å¸¸ï¼š

```bash
# æ£€æŸ¥ libvirtd æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ
sudo systemctl status libvirtd

# (æ— éœ€ sudo) å°è¯•åˆ—å‡ºè™šæ‹Ÿæœºï¼ˆç°åœ¨åº”è¯¥æ˜¯ç©ºçš„ï¼‰
virsh list --all
```

å¦‚æœ `virsh list` å‘½ä»¤åœ¨æ²¡æœ‰ `sudo` çš„æƒ…å†µä¸‹æˆåŠŸè¿è¡Œï¼ˆå³ä½¿åªæ˜¯æ˜¾ç¤ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼‰ï¼Œé‚£ä¹ˆæ‚¨çš„å®¿ä¸»æœºå°±å‡†å¤‡å¥½äº†ï¼

æ‚¨ç°åœ¨ä¹Ÿå¯ä»¥åœ¨åº”ç”¨ç¨‹åºèœå•ä¸­æ‰¾åˆ°å¹¶å¯åŠ¨ "Virtual Machine Manager" (`virt-manager`) å›¾å½¢ç•Œé¢ã€‚

-----

è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼€å§‹ã€‚å½“æ‚¨å®Œæˆå®¿ä¸»æœºçš„å‡†å¤‡å·¥ä½œåï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›\*\*åˆ›å»ºâ€œé»„é‡‘é•œåƒâ€è™šæ‹Ÿæœºï¼ˆç¬¬äºŒé˜¶æ®µï¼‰\*\*çš„è¯¦ç»†æ­¥éª¤ã€‚

å¥½çš„ï¼Œæˆ‘ä»¬è¿›å…¥**é˜¶æ®µäºŒï¼šåˆ›å»ºâ€œé»„é‡‘é•œåƒâ€è™šæ‹Ÿæœº**ã€‚

è¿™ä¸ªâ€œé»„é‡‘é•œåƒâ€æ˜¯ä¸€ä¸ªé¢„å…ˆé…ç½®å¥½çš„è™šæ‹Ÿæœºæ¨¡æ¿ã€‚æˆ‘ä»¬å°†åªåˆ›å»ºå’Œé…ç½® *ä¸€æ¬¡*ï¼Œç„¶åå…‹éš†å®ƒæ¥åˆ›å»ºæ‰€æœ‰çš„ Hadoop èŠ‚ç‚¹ï¼ˆNameNode, DataNodeï¼‰ã€‚è¿™ä¼šèŠ‚çœå¤§é‡é‡å¤åŠ³åŠ¨ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨å›¾å½¢åŒ–çš„ `virt-manager` å·¥å…·ï¼Œå› ä¸ºå®ƒæ›´ç›´è§‚ã€‚

-----

### é˜¶æ®µäºŒï¼šåˆ›å»ºâ€œé»„é‡‘é•œåƒâ€è™šæ‹Ÿæœºï¼ˆè¯¦ç»†æ­¥éª¤ï¼‰

#### 1\. (å®¿ä¸»æœº) ä¸‹è½½æ“ä½œç³»ç»Ÿé•œåƒ

Hadoop åœ¨ Linux ä¸Šè¿è¡Œè‰¯å¥½ã€‚æˆ‘å¼ºçƒˆæ¨è **Ubuntu Server 22.04 LTS**ï¼ˆé•¿æœŸæ”¯æŒç‰ˆï¼‰ï¼Œå®ƒç¨³å®šä¸”èµ„æºä¸°å¯Œã€‚

> **æç¤º**ï¼šæ‚¨å¯ä»¥åœ¨å®¿ä¸»æœºä¸Šæ‰“å¼€æµè§ˆå™¨ä¸‹è½½ï¼Œæˆ–è€…ä½¿ç”¨ `wget`ï¼š
>
> ```bash
> # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹é“¾æ¥ï¼Œæ‚¨å¯ä»¥å» Ubuntu å®˜ç½‘è·å–æœ€æ–°çš„
> cd ~/Downloads  # æˆ–è€…æ‚¨æƒ³å­˜æ”¾ ISO çš„ä»»ä½•åœ°æ–¹
> wget https://releases.ubuntu.com/22.04/ubuntu-22.04.4-live-server-amd64.iso
> ```

#### 2\. (å®¿ä¸»æœº) å¯åŠ¨ Virt-Manager å¹¶åˆ›å»º VM

1.  ä»æ‚¨çš„åº”ç”¨ç¨‹åºèœå•å¯åŠ¨ "Virtual Machine Manager"ã€‚
2.  ç‚¹å‡»å·¦ä¸Šè§’çš„ "Create a new virtual machine" å›¾æ ‡ï¼ˆåƒä¸€ä¸ªå‘å…‰çš„æ˜¾ç¤ºå™¨ï¼‰ã€‚
3.  **æ­¥éª¤ 1/4: New VM**
      * é€‰æ‹© "Local install media (ISO image or CDROM)"ã€‚
      * ç‚¹å‡» "Forward"ã€‚
4.  **æ­¥éª¤ 2/4: Locate media**
      * ç‚¹å‡» "Browse..." -\> "Browse Local"ã€‚
      * å¯¼èˆªåˆ°æ‚¨åˆšåˆšä¸‹è½½çš„ Ubuntu Server ISO æ–‡ä»¶å¹¶é€‰æ‹©å®ƒã€‚
      * ç¡®ä¿ "Automatically detect OS based on media" è¢«å‹¾é€‰ï¼ˆå®ƒåº”è¯¥ä¼šè‡ªåŠ¨è¯†åˆ«ä¸º Ubuntu 22.04ï¼‰ã€‚
      * ç‚¹å‡» "Forward"ã€‚
5.  **æ­¥éª¤ 3/4: Choose Memory and CPU**
      * **Memory (RAM):** è‡³å°‘ `2048` MB (å³ 2GB)ã€‚å¦‚æœæ‚¨çš„å®¿ä¸»æœºå†…å­˜å……è¶³ï¼Œ`4096` MB (4GB) æ›´å¥½ã€‚
      * **CPUs:** `2` ä¸ª vCPUs å³å¯ã€‚
      * ç‚¹å‡» "Forward"ã€‚
6.  **æ­¥éª¤ 4/4: Create Storage**
      * é€‰æ‹© "Create a disk image for the virtual machine"ã€‚
      * ä¸ºæ¨¡æ¿è®¾ç½®å¤§å°ï¼š`20` GB è¶³å¤Ÿäº†ã€‚
      * ç‚¹å‡» "Forward"ã€‚
7.  **æœ€åä¸€æ­¥: Ready to begin**
      * **Name:** ç»™æ‚¨çš„VMèµ·ä¸€ä¸ªæè¿°æ€§çš„åå­—ï¼Œä¾‹å¦‚ `hadoop-template` æˆ– `ubuntu-golden`ã€‚
      * **é‡è¦ï¼š** å‹¾é€‰ "Customize configuration before install"ã€‚
      * ç‚¹å‡» "Finish"ã€‚

#### 3\. (å®¿ä¸»æœº) å…³é”®é…ç½®ï¼šç½‘ç»œ

åœ¨å®‰è£…å¼€å§‹å‰ï¼Œ`virt-manager` ä¼šæ˜¾ç¤ºä¸€ä¸ªé…ç½®çª—å£ã€‚æˆ‘ä»¬æ¥æ£€æŸ¥ç½‘ç»œè®¾ç½®ã€‚

1.  åœ¨å·¦ä¾§åˆ—è¡¨ä¸­ï¼Œç‚¹å‡» "NIC" (æˆ– "Network")ã€‚
2.  ç¡®ä¿ **Network source** è®¾ç½®ä¸º "**NAT (default)**"ã€‚
      * *è¯´æ˜ï¼š* è¿™å°†ä½¿æ‚¨çš„ VM å¯ä»¥é€šè¿‡å®¿ä¸»æœºè®¿é—®å¤–éƒ¨äº’è”ç½‘ï¼ˆä»¥ä¸‹è½½ Java å’Œ Hadoopï¼‰ï¼Œä½†å®ƒç›®å‰è¿˜ä¸èƒ½è¢«å…¶ä»– VM è®¿é—®ã€‚æˆ‘ä»¬ä¼šåœ¨ä¸‹ä¸€é˜¶æ®µè§£å†³è¿™ä¸ªé—®é¢˜ã€‚
3.  ç‚¹å‡»å·¦ä¸Šè§’çš„ "Begin Installation"ã€‚

#### 4\. (VM å†…éƒ¨) å®‰è£… Ubuntu Server

VM å°†å¯åŠ¨å¹¶åŠ è½½ Ubuntu Server å®‰è£…ç¨‹åºã€‚æ‚¨ç°åœ¨æ˜¯åœ¨è™šæ‹Ÿæœºæ§åˆ¶å°å†…æ“ä½œã€‚

1.  **è¯­è¨€ï¼š** é€‰æ‹© Englishï¼ˆæ¨èç”¨äºæœåŠ¡å™¨ï¼‰æˆ–ä¸­æ–‡ã€‚
2.  **é”®ç›˜ï¼š** æŒ‰ç…§é»˜è®¤è®¾ç½®ã€‚
3.  **ç½‘ç»œè¿æ¥ï¼š** ä¿æŒé»˜è®¤ï¼ˆDHCPï¼‰ï¼Œå®ƒåº”è¯¥ä¼šè‡ªåŠ¨è·å–ä¸€ä¸ª IP åœ°å€ã€‚
4.  **ä»£ç†ï¼š** ç•™ç©º (æŒ‰ Done)ã€‚
5.  **é•œåƒæºï¼š** ä¿æŒé»˜è®¤ (æŒ‰ Done)ã€‚
6.  **å­˜å‚¨ï¼š** é€‰æ‹© "Use an entire disk" å¹¶æŒ‰ Doneã€‚åœ¨ä¸‹ä¸€ä¸ªç¡®è®¤é¡µé¢å†æ¬¡æŒ‰ Doneã€‚
7.  **Profile Setup (ç”¨æˆ·ä¿¡æ¯):**
      * Your name: `Hadoop Admin` (æˆ–ä»»æ„)
      * Your server's name: `hadoop-template`
      * Pick a username: `hadoop` (**æ¨è**ï¼šç›´æ¥åˆ›å»º `hadoop` ç”¨æˆ·)
      * Choose a password: (è®¾ç½®ä¸€ä¸ªæ‚¨èƒ½è®°ä½çš„å¼ºå¯†ç )
8.  **SSH Setup (éå¸¸é‡è¦):**
      * **å‹¾é€‰ "Install OpenSSH server"**ã€‚è¿™æ˜¯å¿…é¡»çš„ï¼ŒHadoop ä¾èµ– SSH æ¥ç®¡ç†èŠ‚ç‚¹ã€‚
9.  **Featured Server Snaps:**
      * **ä¸è¦**é€‰æ‹© "hadoop"ã€‚æˆ‘ä»¬å°†æ‰‹åŠ¨å®‰è£…ï¼Œä»¥è·å¾—å®Œå…¨æ§åˆ¶æƒã€‚
      * ä¿æŒæ‰€æœ‰é€‰é¡¹éƒ½ä¸å‹¾é€‰ï¼Œç›´æ¥æŒ‰ Doneã€‚
10. **ç­‰å¾…å®‰è£…å®Œæˆ**... ç„¶åé€‰æ‹© "Reboot Now"ã€‚

> **æç¤ºï¼š** å½“å®ƒæ˜¾ç¤º "Please remove the installation medium" æ—¶ï¼Œ**ä¸è¦**ç®¡å®ƒã€‚`virt-manager` ä¼šè‡ªåŠ¨å¤„ç†ã€‚ç›´æ¥æŒ‰ Enter é”®ã€‚

#### 5\. (VM å†…éƒ¨) é…ç½®â€œé»„é‡‘é•œåƒâ€

VM é‡å¯åï¼Œä½¿ç”¨æ‚¨åˆ›å»ºçš„ `hadoop` ç”¨æˆ·å’Œå¯†ç ç™»å½•ã€‚

1.  **æ›´æ–°ç³»ç»Ÿå¹¶å®‰è£… Java**ï¼šHadoop éœ€è¦ Java è¿è¡Œç¯å¢ƒã€‚

    ```bash
    # åˆ·æ–°è½¯ä»¶åŒ…åˆ—è¡¨
    sudo apt update

    # å®‰è£… OpenJDK 11 (Hadoop 3.x å®Œç¾æ”¯æŒ)
    # -y (è‡ªåŠ¨å›ç­” yes)
    # -headless (æ— å¤´ç‰ˆï¼ŒæœåŠ¡å™¨ä¸éœ€è¦ GUI)
    sudo apt install -y openjdk-11-jdk-headless

    # éªŒè¯å®‰è£…
    java -version
    # æ‚¨åº”è¯¥ä¼šçœ‹åˆ° OpenJDK 11 çš„è¾“å‡º
    ```

2.  **å®‰è£…æœ‰ç”¨çš„å·¥å…·** (æ¨è)ï¼š

    ```bash
    # net-tools åŒ…å« ifconfig, rsync ç”¨äºæ–‡ä»¶åŒæ­¥
    sudo apt install -y net-tools rsync
    ```

3.  **å…³é—­é˜²ç«å¢™ (ä»…é™æµ‹è¯•ç¯å¢ƒ)**ï¼š

    ä¸ºäº†ç®€åŒ–æˆ‘ä»¬è¿™ä¸ªå­¦ä¹ é›†ç¾¤çš„å†…éƒ¨ç½‘ç»œé€šä¿¡ï¼Œæˆ‘ä»¬ç¦ç”¨ `ufw` é˜²ç«å¢™ã€‚

    ```bash
    # æ£€æŸ¥çŠ¶æ€ (åº”è¯¥æ˜¯ active)
    sudo ufw status

    # ç¦ç”¨é˜²ç«å¢™
    sudo ufw disable
    ```

    > *æ³¨æ„ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨æ°¸è¿œä¸åº”è¯¥è¿™æ ·åšï¼Œè€Œæ˜¯åº”è¯¥é…ç½®ä¸¥æ ¼çš„é˜²ç«å¢™è§„åˆ™ã€‚*

#### 6\. æ¸…ç†å’Œå…³æœº

æœ€åï¼Œæˆ‘ä»¬æ¸…ç†ä¸€ä¸‹æ¨¡æ¿å¹¶å…³é—­å®ƒï¼Œå‡†å¤‡è¿›è¡Œå…‹éš†ã€‚

```bash
# æ¸…ç†ä¸‹è½½çš„è½¯ä»¶åŒ…ç¼“å­˜
sudo apt clean

# å…³é—­è™šæ‹Ÿæœº
sudo shutdown now
```

-----

æ­å–œï¼æ‚¨ç°åœ¨æœ‰äº†ä¸€ä¸ªåä¸º `hadoop-template` ä¸”å·²å…³é—­çš„è™šæ‹Ÿæœºã€‚å®ƒå·²ç»å®‰è£…äº† Ubuntu Serverã€OpenSSH å’Œ Javaã€‚

æˆ‘ä»¬å·²ç»å®Œæˆäº†é˜¶æ®µäºŒã€‚å½“æ‚¨å‡†å¤‡å¥½åï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä»¬å°†è¿›å…¥**é˜¶æ®µä¸‰ï¼šå…‹éš†è™šæ‹Ÿæœºå’Œé…ç½®ç½‘ç»œ**ã€‚




### é˜¶æ®µä¸‰ï¼šå…‹éš†è™šæ‹Ÿæœºä¸é…ç½®ç½‘ç»œï¼ˆè¯¦ç»†æ­¥éª¤ï¼‰

#### 1\. (å®¿ä¸»æœº) å…‹éš†è™šæ‹Ÿæœº

æˆ‘ä»¬å°†ä½¿ç”¨ `virt-manager` å…‹éš†æ‚¨çš„ `hadoop-template` ä¸‰æ¬¡ã€‚

1.  æ‰“å¼€ `virt-manager`ã€‚
2.  ç¡®ä¿ `hadoop-template` **å·²å…³æœº**ã€‚
3.  å³é”®ç‚¹å‡» `hadoop-template` -\> **Clone**ã€‚
4.  åœ¨å¼¹å‡ºçš„çª—å£ä¸­ï¼š
      * **Name:** `hadoop-namenode`
      * **Storage:** å‹¾é€‰ "Create a full copy of the disk" (é‡è¦ï¼)ã€‚
5.  ç‚¹å‡» **Clone**ã€‚
6.  **é‡å¤æ­¤è¿‡ç¨‹** ä¸¤æ¬¡ï¼Œåˆ›å»ºï¼š
      * `hadoop-datanode1`
      * `hadoop-datanode2`

æ‚¨ç°åœ¨åº”è¯¥æœ‰å››ä¸ªå·²å…³æœºçš„ VMï¼š`hadoop-template`ï¼ˆæˆ‘ä»¬çš„å¤‡ä»½ï¼‰å’Œä¸‰ä¸ªæ–°èŠ‚ç‚¹ã€‚

#### 2\. (VM å†…éƒ¨) å¯åŠ¨å¹¶é…ç½®æ¯ä¸ªèŠ‚ç‚¹

æˆ‘ä»¬å°†**ä¾æ¬¡**å¯åŠ¨å’Œé…ç½®**æ¯ä¸€ä¸ª**æ–°çš„ VM (`hadoop-namenode`, `hadoop-datanode1`, `hadoop-datanode2`)ã€‚

**è¯·å¯¹ä»¥ä¸‹ä¸‰ä¸ª VM åˆ†åˆ«æ‰§è¡Œ 2a, 2b å’Œ 2c æ­¥éª¤ï¼š**

##### a. å¯åŠ¨å¹¶è®¾ç½®ä¸»æœºå

1.  åœ¨ `virt-manager` ä¸­ï¼Œå¯åŠ¨ `hadoop-namenode` è™šæ‹Ÿæœºã€‚

2.  ä½¿ç”¨æ‚¨åœ¨æ¨¡æ¿ä¸­åˆ›å»ºçš„ `hadoop` ç”¨æˆ·ç™»å½•ã€‚

3.  **è®¾ç½®ä¸»æœºåï¼š** VM çš„å†…éƒ¨ä¸»æœºåä»ç„¶æ˜¯ `hadoop-template`ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ”¹å®ƒã€‚

    ```bash
    # (åœ¨ hadoop-namenode VM å†…éƒ¨)
    sudo hostnamectl set-hostname hadoop-namenode
    ```

> **å¯¹ `hadoop-datanode1` é‡å¤æ­¤æ“ä½œï¼š**
> `sudo hostnamectl set-hostname hadoop-datanode1`
>
> **å¯¹ `hadoop-datanode2` é‡å¤æ­¤æ“ä½œï¼š**
> `sudo hostnamectl set-hostname hadoop-datanode2`

##### b. é…ç½®é™æ€ IP (Netplan)

è¿™æ˜¯æœ€å¤æ‚ä½†æœ€é‡è¦çš„ä¸€æ­¥ã€‚æˆ‘ä»¬å°†æŠŠé»˜è®¤çš„ DHCPï¼ˆè‡ªåŠ¨è·å–IPï¼‰æ”¹ä¸ºé™æ€IPã€‚

1.  **æ‰¾å‡ºç½‘å¡åç§°ï¼š**

    ```bash
    # (åœ¨ VM å†…éƒ¨)
    ip a
    ```

    æŸ¥çœ‹è¾“å‡ºã€‚æ‚¨ä¼šçœ‹åˆ° `lo` (æœ¬åœ°å›ç¯) å’Œå¦ä¸€ä¸ªæ¥å£ï¼Œé€šå¸¸åå­—æ˜¯ `ens3`ã€`enp1s0` æˆ– `eth0`ã€‚è®°ä¸‹è¿™ä¸ªåå­—ï¼ˆæˆ‘ä»¬å‡è®¾å®ƒæ˜¯ `ens3`ï¼‰ã€‚

2.  **ç¼–è¾‘ Netplan é…ç½®æ–‡ä»¶ï¼š**
    Ubuntu ä½¿ç”¨ `netplan` ç®¡ç†ç½‘ç»œã€‚é…ç½®æ–‡ä»¶ä½äº `/etc/netplan/`ã€‚

    ```bash
    # (åœ¨ VM å†…éƒ¨)
    # æ³¨æ„ï¼šæ‚¨çš„æ–‡ä»¶åå¯èƒ½æ˜¯ 00-installer-config.yaml æˆ– 50-cloud-init.yaml ç­‰
    sudo nano /etc/netplan/00-installer-config.yaml 
    ```
    ç¬¬ä¸€æ­¥ï¼šæ°¸ä¹…ç¦ç”¨ Cloud-Init çš„ç½‘ç»œç®¡ç† æˆ‘ä»¬æ¥å‘Šè¯‰ cloud-initï¼šâ€œåˆ«å†ç®¡ç½‘ç»œäº†ï¼â€

```Bash

# (åœ¨ VM å†…éƒ¨)
# 1. åˆ›å»ºä¸€ä¸ªæ–°çš„é…ç½®æ–‡ä»¶æ¥è¦†ç›–é»˜è®¤è®¾ç½®
echo "network: {config: disabled}" | sudo tee /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg

# 2. åˆ é™¤ cloud-init ä¹‹å‰ç”Ÿæˆçš„æ—§é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ä½†æ¨èï¼‰
sudo rm /etc/netplan/50-cloud-init.yaml
```

ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„ Netplan é…ç½®æ–‡ä»¶
ç°åœ¨ cloud-init ä¸ä¼šå†æ£ä¹±äº†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå®ƒä¸ä¼šè§¦ç¢°çš„æ–°é…ç½®æ–‡ä»¶ã€‚

æ³¨æ„ï¼š nano å¯¹ç¼©è¿›å¾ˆæ•æ„Ÿã€‚è¯·ç¡®ä¿ç©ºæ ¼æ­£ç¡®ã€‚

åœ¨ hadoop-datanode1 ä¸Šï¼š

```Bash

# (åœ¨ datanode1 ä¸Š)
sudo nano /etc/netplan/01-hadoop-static.yaml
```

3.  **ä¿®æ”¹æ–‡ä»¶å†…å®¹ï¼š**
    æ‚¨çš„æ–‡ä»¶**ä¹‹å‰**å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼ˆä½¿ç”¨ DHCPï¼‰ï¼š

    ```yaml
    # ä¹‹å‰ (DHCP):
    network:
      ethernets:
        ens3: # <== ä½ çš„ç½‘å¡åç§°
          dhcp4: true
      version: 2
    ```

    **è¯·å°†å…¶ä¿®æ”¹ä¸º**é™æ€é…ç½®ã€‚**æ³¨æ„ YAML æ ¼å¼å¯¹ç¼©è¿›éå¸¸æ•æ„Ÿï¼**

    **å¯¹äº `hadoop-namenode` (192.168.122.101):**

```yaml
network:
  ethernets:
    ens3: # <== æ‚¨çš„ç½‘å¡åç§°
      dhcp4: no
      addresses:
        - 192.168.122.101/24  # <-- èŠ‚ç‚¹çš„ IP
      
      # gateway4: 192.168.122.1  <-- è¿™æ˜¯å·²å¼ƒç”¨çš„æ—§æ–¹æ³•

      # è¿™æ˜¯æ–°çš„ã€æ¨èçš„æ–¹æ³•
      routes:
        - to: default
          via: 192.168.122.1  # <-- 192.168.122.1 æ˜¯ KVM/libvirt çš„é»˜è®¤ç½‘å…³

      nameservers:
        addresses: [192.168.122.1, 8.8.8.8]
  version: 2
```

      * `192.168.122.1` æ˜¯ `virt-manager` é»˜è®¤NATç½‘ç»œçš„ç½‘å…³ã€‚
      * `8.8.8.8` æ˜¯ Google çš„ DNSï¼Œç¡®ä¿VMå¯ä»¥è®¿é—®äº’è”ç½‘ã€‚

    **å¯¹äº `hadoop-datanode1` (192.168.122.102):**
    ä½¿ç”¨ `addresses: [192.168.122.102/24]` (å…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜)ã€‚

    **å¯¹äº `hadoop-datanode2` (192.168.122.103):**
    ä½¿ç”¨ `addresses: [192.168.122.103/24]` (å…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜)ã€‚

4.  **åº”ç”¨ç½‘ç»œé…ç½®ï¼š**
    ä¿å­˜æ–‡ä»¶åï¼Œåœ¨ **æ¯ä¸ª** VM ä¸Šè¿è¡Œï¼š

    ```bash
    # (åœ¨ VM å†…éƒ¨)
    sudo netplan apply
    ```

    æ‚¨çš„ SSH è¿æ¥å¯èƒ½ä¼šæ–­å¼€ï¼ˆå¦‚æœä½¿ç”¨ SSHï¼‰ã€‚åœ¨ `virt-manager` æ§åˆ¶å°ä¸­ï¼Œä½¿ç”¨ `ip a` éªŒè¯æ–°çš„ IP åœ°å€æ˜¯å¦å·²ç”Ÿæ•ˆã€‚

##### c. é…ç½® hosts æ–‡ä»¶ (DNS è§£æ)

æœ€åä¸€æ­¥æ˜¯è®©æ‰€æœ‰ VM éƒ½èƒ½é€šè¿‡ä¸»æœºåæ‰¾åˆ°å½¼æ­¤ã€‚**æ‚¨å¿…é¡»åœ¨ *æ‰€æœ‰ä¸‰ä¸ª* VM ä¸Šæ‰§è¡Œæ­¤æ“ä½œã€‚**

1.  ç¼–è¾‘ `/etc/hosts` æ–‡ä»¶ï¼š

    ```bash
    # (åœ¨ VM å†…éƒ¨)
    sudo nano /etc/hosts
    ```

2.  åœ¨æ–‡ä»¶**é¡¶éƒ¨**æ·»åŠ ä»¥ä¸‹ä¸‰è¡Œï¼š

    ```
    # Hadoop Cluster
    192.168.122.101  hadoop-namenode
    192.168.122.102  hadoop-datanode1
    192.168.122.103  hadoop-datanode2
    ```

    (ä¿ç•™ `127.0.0.1 localhost` ç­‰å…¶ä»–é»˜è®¤æ¡ç›®)

#### 3\. (VM å†…éƒ¨) éªŒè¯å’Œé‡å¯

1.  **éªŒè¯ï¼š**
    ç°åœ¨æ‚¨åº”è¯¥åœ¨ `hadoop-namenode` ä¸Šäº†ã€‚å°è¯• `ping` å…¶ä»–èŠ‚ç‚¹ï¼š

    ```bash
    # (åœ¨ hadoop-namenode VM å†…éƒ¨)
    ping hadoop-datanode1
    ping hadoop-datanode2
    ```

    å¦‚æœå®ƒä»¬éƒ½èƒ½ `ping` é€šå¹¶ä¸”æ˜¾ç¤ºäº†æ­£ç¡®çš„ IP åœ°å€ï¼Œé‚£ä¹ˆæ‚¨çš„ç½‘ç»œå°±é…ç½®æˆåŠŸäº†ï¼

2.  **é‡å¯ï¼š**
    ä¸ºç¡®ä¿æ‰€æœ‰æ›´æ”¹ï¼ˆä¸»æœºåã€ç½‘ç»œï¼‰åœ¨é‡å¯åéƒ½èƒ½ç”Ÿæ•ˆï¼Œè¯·åœ¨**æ‰€æœ‰ä¸‰ä¸ª** VM ä¸Šæ‰§è¡Œï¼š

    ```bash
    # (åœ¨æ‰€æœ‰ä¸‰ä¸ª VM å†…éƒ¨)
    sudo reboot
    ```

-----

é˜¶æ®µä¸‰å®Œæˆï¼æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªç”±ä¸‰ä¸ªè™šæ‹Ÿæœºç»„æˆçš„é›†ç¾¤ï¼Œå®ƒä»¬å…·æœ‰æ°¸ä¹…çš„ä¸»æœºåå’Œé™æ€IPåœ°å€ï¼Œå¹¶ä¸”å¯ä»¥ç›¸äº’é€šä¿¡ã€‚

è¿™ä¸ºå®‰è£… Hadoop å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚å‡†å¤‡å¥½åï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä»¬å°†å¼€å§‹**é˜¶æ®µå››ï¼šHadoop å®‰è£…ä¸é…ç½®**ã€‚


å¥½çš„ï¼Œæˆ‘ä»¬è¿›å…¥å…³é”®çš„**é˜¶æ®µå››ï¼šHadoop å®‰è£…ä¸é…ç½®**ã€‚

ä»ç°åœ¨å¼€å§‹ï¼Œè¯·**ä¸è¦å†ä½¿ç”¨ `virt-manager` çš„é»‘å±æ§åˆ¶å°**äº†ã€‚

**æ‚¨çš„å·¥ä½œæµç¨‹ï¼š** æ‰“å¼€**ä¸‰ä¸ª**å®¿ä¸»æœºç»ˆç«¯çª—å£ã€‚

  * **ç»ˆç«¯ 1:** `ssh hadoop@hadoop-namenode`
  * **ç»ˆç«¯ 2:** `ssh hadoop@hadoop-datanode1`
  * **ç»ˆç«¯ 3:** `ssh hadoop@hadoop-datanode2`

æˆ‘ä»¬å°†ä½¿ç”¨ `(ALL)`ã€`(NameNode)` æˆ– `(DataNodes)` æ¥æ ‡è®°å‘½ä»¤åº”è¯¥åœ¨å“ªäº›ç»ˆç«¯ä¸­æ‰§è¡Œã€‚

-----

### é˜¶æ®µå››ï¼šHadoop å®‰è£…ä¸é…ç½®

#### 1\. (ALL) ä¸‹è½½å¹¶è§£å‹ Hadoop

æˆ‘ä»¬åœ¨æ‰€æœ‰ä¸‰ä¸ªèŠ‚ç‚¹ä¸Šä¸‹è½½ Hadoopã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Hadoop 3.3.6ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç¨³å®šçš„ç‰ˆæœ¬ã€‚

> **(ALL)** åœ¨æ‰€æœ‰ä¸‰ä¸ªç»ˆç«¯ä¸­è¿è¡Œï¼š

```bash
# åˆ‡æ¢åˆ° hadoop ç”¨æˆ·çš„ä¸»ç›®å½•
cd ~

# ä¸‹è½½ Hadoop 3.3.6 äºŒè¿›åˆ¶åŒ…
wget https://mirrors.dotsrc.org/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# è§£å‹
tar -xzf hadoop-3.3.6.tar.gz

# å°†å…¶ç§»åŠ¨åˆ° /usr/local/ ç›®å½•ä¸‹ï¼Œå¹¶é‡å‘½åä¸º hadoop
sudo mv hadoop-3.3.6 /usr/local/hadoop

# æ›´æ”¹ hadoop ç›®å½•çš„æ‰€æœ‰æƒï¼Œå½’ hadoop ç”¨æˆ·æ‰€æœ‰
sudo chown -R hadoop:hadoop /usr/local/hadoop
```

#### 2\. (ALL) è®¾ç½®ç¯å¢ƒå˜é‡

æˆ‘ä»¬éœ€è¦å‘Šè¯‰ç³»ç»Ÿ Java å’Œ Hadoop åœ¨å“ªé‡Œã€‚

> **(ALL)** åœ¨æ‰€æœ‰ä¸‰ä¸ªç»ˆç«¯ä¸­è¿è¡Œï¼š

```bash
# æ‰“å¼€ .bashrc æ–‡ä»¶è¿›è¡Œç¼–è¾‘
nano ~/.bashrc
```

æ»šåŠ¨åˆ°æ–‡ä»¶çš„**æœ€åº•éƒ¨**ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```bash
# Java Home
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Hadoop Home
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

ä¿å­˜æ–‡ä»¶ (Ctrl+O) å¹¶é€€å‡º (Ctrl+X)ã€‚ç„¶åï¼Œ**ç«‹å³ç”Ÿæ•ˆ**è¿™äº›å˜é‡ï¼š

```bash
# (ALL) è¿è¡Œ
source ~/.bashrc

# (ALL) éªŒè¯ (å¯é€‰)
echo $HADOOP_HOME
# åº”è¯¥è¾“å‡º: /usr/local/hadoop
```

#### 3\. (ALL) é…ç½® hadoop-env.sh

Hadoop éœ€è¦åœ¨å…¶è‡ªå·±çš„é…ç½®æ–‡ä»¶ä¸­æ˜ç¡®çŸ¥é“ `JAVA_HOME` çš„è·¯å¾„ã€‚

> **(ALL)** åœ¨æ‰€æœ‰ä¸‰ä¸ªç»ˆç«¯ä¸­è¿è¡Œï¼š

```bash
# ç¼–è¾‘ hadoop-env.sh æ–‡ä»¶
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
```

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ°ï¼ˆæˆ–`Ctrl+W`æœç´¢ï¼‰`export JAVA_HOME=` è¿™ä¸€è¡Œã€‚å®ƒå¯èƒ½è¢«æ³¨é‡Šæ‰äº†ï¼ˆä»¥ `#` å¼€å¤´ï¼‰æˆ–è€…æŒ‡å‘ä¸€ä¸ªå˜é‡ã€‚

è¯·å°†å…¶ä¿®æ”¹ä¸º**æ˜ç¡®çš„è·¯å¾„**ï¼ˆåˆ é™¤ `#`ï¼‰ï¼š

```bash
# (å¤§çº¦åœ¨ç¬¬ 54 è¡Œ)
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

ä¿å­˜å¹¶é€€å‡ºã€‚

-----

#### 4\. (NameNode) è®¾ç½®æ— å¯†ç  SSH

è¿™æ˜¯**æœ€å…³é”®**çš„ä¸€æ­¥ã€‚NameNode éœ€è¦èƒ½å¤Ÿ**æ— éœ€å¯†ç **å°±é€šè¿‡ SSH ç™»å½•åˆ°æ‰€æœ‰ DataNodeï¼ˆä»¥åŠå®ƒè‡ªå·±ï¼‰æ¥å¯åŠ¨å’Œåœæ­¢æœåŠ¡ã€‚

> **(NameNode)** **åªåœ¨ `hadoop-namenode` ç»ˆç«¯**ä¸­è¿è¡Œï¼š

```bash
# 1. ç”Ÿæˆ SSH å¯†é’¥å¯¹ï¼ˆå¦‚æœä¹‹å‰æ²¡ç”Ÿæˆè¿‡ï¼‰
# ä¸€è·¯æŒ‰ Enter é”®æ¥å—æ‰€æœ‰é»˜è®¤å€¼ï¼ˆå°¤å…¶æ˜¯â€œno passphraseâ€ï¼‰
ssh-keygen -t rsa

# 2. å°†å…¬é’¥å¤åˆ¶åˆ°é›†ç¾¤ä¸­çš„ *æ‰€æœ‰* èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬å®ƒè‡ªå·±ï¼‰
ssh-copy-id hadoop@hadoop-namenode
ssh-copy-id hadoop@hadoop-datanode1
ssh-copy-id hadoop@hadoop-datanode2
```

  * æ¯æ¬¡ `ssh-copy-id` éƒ½ä¼šè¦æ±‚æ‚¨è¾“å…¥ `hadoop` ç”¨æˆ·çš„å¯†ç ã€‚
  * å®ƒå¯èƒ½ä¼šè¯¢é—® "Are you sure you want to continue connecting (yes/no)?"ï¼Œè¾“å…¥ `yes`ã€‚

**éªŒè¯ï¼** è¿™ä¸€æ­¥å¿…é¡»æˆåŠŸï¼š

```bash
# (NameNode) å°è¯•ç™»å½•åˆ° datanode1
ssh hadoop-datanode1
# æ‚¨åº”è¯¥ä¼š *ç«‹å³* ç™»å½•ï¼Œè€Œ *ä¸* éœ€è¦è¾“å…¥å¯†ç ã€‚
# è¾“å…¥ exit é€€å‡º
exit

# (NameNode) å°è¯•ç™»å½•åˆ° datanode2
ssh hadoop-datanode2
# åŒæ ·ï¼Œåº”è¯¥ç«‹å³ç™»å½•ã€‚
# è¾“å…¥ exit é€€å‡º
exit
```

å¦‚æœä¸éœ€è¦å¯†ç å°±èƒ½ç™»å½•ï¼Œæ­å–œæ‚¨ï¼Œæœ€éš¾çš„éƒ¨åˆ†ç»“æŸäº†ï¼

-----

#### 5\. (NameNode) ç¼–è¾‘æ ¸å¿ƒé…ç½®æ–‡ä»¶

æˆ‘ä»¬å°†**åªåœ¨ NameNode ä¸Š**ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œç„¶åå°†å®ƒä»¬åˆ†å‘åˆ°å…¶ä»–èŠ‚ç‚¹ã€‚

> **(NameNode)** **åªåœ¨ `hadoop-namenode` ç»ˆç«¯**ä¸­æ“ä½œã€‚

æ‰€æœ‰é…ç½®æ–‡ä»¶éƒ½åœ¨ `$HADOOP_HOME/etc/hadoop/` ç›®å½•ä¸­ã€‚

##### a. core-site.xml

```bash
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

åœ¨ `<configuration>` å’Œ `</configuration>` æ ‡ç­¾ä¹‹é—´æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop-namenode:9000</value>
    </property>
</configuration>
```

##### b. hdfs-site.xml

åœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬åˆ›å»º HDFS å®é™…å­˜å‚¨æ•°æ®çš„ç›®å½•ã€‚

```bash
# (NameNode) åªåœ¨ NameNode ä¸Šåˆ›å»º *namenode* ç›®å½•
sudo mkdir -p /usr/local/hadoop/data/namenode
sudo chown -R hadoop:hadoop /usr/local/hadoop/data

# (DataNodes) åªåœ¨ datanode1 å’Œ datanode2 ä¸Šåˆ›å»º *datanode* ç›®å½•
# è¯·åœ¨æ‚¨çš„ *ç»ˆç«¯ 2* å’Œ *ç»ˆç«¯ 3* ä¸­è¿è¡Œè¿™ä¸¤ä¸ªå‘½ä»¤
sudo mkdir -p /usr/local/hadoop/data/datanode
sudo chown -R hadoop:hadoop /usr/local/hadoop/data
```

ç°åœ¨ï¼Œå›åˆ° **NameNode** ç»ˆç«¯ï¼Œç¼–è¾‘ `hdfs-site.xml`ï¼š

```bash
# (NameNode)
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

åœ¨ `<configuration>` æ ‡ç­¾ä¹‹é—´æ·»åŠ ï¼š

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/data/datanode</value>
    </property>
</configuration>
```

##### c. mapred-site.xml

æ­¤æ–‡ä»¶é»˜è®¤ä¸å­˜åœ¨ï¼ŒHadoop æä¾›äº†ä¸€ä¸ªæ¨¡æ¿ã€‚

```bash
# (NameNode) å…ˆä»æ¨¡æ¿å¤åˆ¶
cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template $HADOOP_HOME/etc/hadoop/mapred-site.xml

# (NameNode) å†ç¼–è¾‘
nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

åœ¨ `<configuration>` æ ‡ç­¾ä¹‹é—´æ·»åŠ ï¼ˆå‘Šè¯‰ MapReduce åœ¨ YARN ä¸Šè¿è¡Œï¼‰ï¼š

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>

</configuration>
```

##### d. yarn-site.xml

```bash
# (NameNode)
nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

```bash
hdfs dfs -mkdir -p /yarn-logs
```

åœ¨ `<configuration>` æ ‡ç­¾ä¹‹é—´æ·»åŠ ï¼ˆé…ç½® YARN æœåŠ¡ï¼‰ï¼š

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop-namenode</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>

    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>hdfs://hadoop-namenode:9000/yarn-logs</value>
    </property>

    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>86400</value> 
    </property>
</configuration>
```

##### e. workers (åŸ slaves æ–‡ä»¶)

è¿™ä¸ªæ–‡ä»¶å‘Šè¯‰ Hadoop å“ªäº›æœºå™¨æ˜¯**å·¥ä½œèŠ‚ç‚¹ (DataNode)**ã€‚

```bash
# (NameNode)
nano $HADOOP_HOME/etc/hadoop/workers
```

åˆ é™¤æ–‡ä»¶ä¸­çš„æ‰€æœ‰å†…å®¹ï¼ˆé»˜è®¤å¯èƒ½æ˜¯ `localhost`ï¼‰ï¼Œç„¶åæ·»åŠ æ‚¨çš„**ä¸¤ä¸ª DataNode** çš„ä¸»æœºåï¼š

```
hadoop-datanode1
hadoop-datanode2
```

-----

#### 6\. (NameNode) åˆ†å‘é…ç½®æ–‡ä»¶

ç°åœ¨æ‚¨çš„ NameNode ä¸Šçš„ `/etc/hadoop/` ç›®å½•å·²ç»é…ç½®å®Œç¾äº†ã€‚æˆ‘ä»¬æŠŠå®ƒä»¬å¤åˆ¶åˆ°æ‰€æœ‰ DataNodeã€‚

> **(NameNode)** **åªåœ¨ `hadoop-namenode` ç»ˆç«¯**ä¸­è¿è¡Œï¼š

```bash
# ä½¿ç”¨ scp (å®‰å…¨å¤åˆ¶) å’Œæˆ‘ä»¬è®¾ç½®å¥½çš„æ— å¯†ç  SSH
# å¤åˆ¶åˆ° datanode1
scp -r $HADOOP_HOME/etc/hadoop/* hadoop-datanode1:$HADOOP_HOME/etc/hadoop/

# å¤åˆ¶åˆ° datanode2
scp -r $HADOOP_HOME/etc/hadoop/* hadoop-datanode2:$HADOOP_HOME/etc/hadoop/
```

-----

**é˜¶æ®µå››å®Œæˆï¼**

æˆ‘ä»¬å·²ç»å®‰è£…äº†æ‰€æœ‰è½¯ä»¶ï¼Œé…ç½®äº†æ‰€æœ‰ XML æ–‡ä»¶ï¼Œå¹¶è®¾ç½®äº† SSHã€‚é›†ç¾¤ç°åœ¨å·²ç»â€œç»„è£…â€å®Œæ¯•ï¼Œä½†å°šæœªå¯åŠ¨ã€‚

å‡†å¤‡å¥½åï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä»¬å°†è¿›å…¥æœ€åä¸€ä¸ªé˜¶æ®µï¼š**é˜¶æ®µäº”ï¼šå¯åŠ¨å’Œæµ‹è¯•é›†ç¾¤**ã€‚

-----

æ‚¨å¯ä»¥è§‚çœ‹è¿™ä¸ªå…³äº[å®‰è£…å¤šèŠ‚ç‚¹ Hadoop é›†ç¾¤çš„è§†é¢‘](https://www.google.com/search?q=https://www.youtube.com/watch%3Fv%3DkYf8J-I-8-w)ï¼Œå®ƒæ¶µç›–äº†æˆ‘ä»¬åˆšåˆšæ‰§è¡Œçš„è®¸å¤šé…ç½®æ­¥éª¤ã€‚

æˆ‘ä»¬æ¥åˆ°äº†æœ€åä¸€ä¸ªé˜¶æ®µï¼è¿™æ˜¯æœ€æ¿€åŠ¨äººå¿ƒçš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å¯åŠ¨æ‰€æœ‰æœåŠ¡å¹¶éªŒè¯æ‚¨çš„é›†ç¾¤æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

ä»ç°åœ¨å¼€å§‹ï¼Œ**æ‰€æœ‰å‘½ä»¤éƒ½åœ¨æ‚¨çš„ `hadoop-namenode` ç»ˆç«¯ï¼ˆ`ssh hadoop@hadoop-namenode`ï¼‰ä¸­è¿è¡Œ**ï¼Œé™¤éç‰¹åˆ«æŒ‡æ˜ã€‚

-----

### ğŸš€ é˜¶æ®µäº”ï¼šå¯åŠ¨ä¸æµ‹è¯•

#### 1\. æ ¼å¼åŒ– HDFS (ä»…é™ç¬¬ä¸€æ¬¡ï¼)

åœ¨é›†ç¾¤ç¬¬ä¸€æ¬¡å¯åŠ¨å‰ï¼Œæ‚¨å¿…é¡»æ ¼å¼åŒ– NameNode ä¸Šçš„ HDFS å­˜å‚¨ã€‚è¿™ä¼šåˆå§‹åŒ–å…ƒæ•°æ®ç›®å½•ã€‚

> **â— è­¦å‘Šï¼š** æ­¤å‘½ä»¤**ä¸€ç”Ÿåªè¿è¡Œä¸€æ¬¡**ï¼
> å¦‚æœæ‚¨åœ¨æ­£åœ¨è¿è¡Œçš„é›†ç¾¤ä¸Šå†æ¬¡è¿è¡Œå®ƒï¼Œ**æ‰€æœ‰ HDFS æ•°æ®éƒ½å°†è¢«æ¸…é™¤**ã€‚

```bash
# (NameNode)
hdfs namenode -format
```

æ‚¨åº”è¯¥ä¼šçœ‹åˆ°å¾ˆå¤šæ—¥å¿—è¾“å‡ºï¼Œè¯·åœ¨æœ€åå¯»æ‰¾ `Storage directory /usr/local/hadoop/data/namenode has been successfully formatted.` è¿™æ¡æ¶ˆæ¯ã€‚

-----

#### 2\. å¯åŠ¨ HDFS æœåŠ¡

æ­¤è„šæœ¬å°†å¯åŠ¨ NameNodeã€SecondaryNameNodeï¼ˆé»˜è®¤åœ¨ NameNode ä¸Šï¼‰ä»¥åŠ `workers` æ–‡ä»¶ä¸­åˆ—å‡ºçš„æ‰€æœ‰ DataNodeã€‚

```bash
# (NameNode)
start-dfs.sh
```

  * å®ƒå¯èƒ½ä¼šè¦æ±‚æ‚¨ç¡®è®¤ SSH æŒ‡çº¹ï¼ˆå¦‚æœè¿™æ˜¯ `localhost` é¦–æ¬¡è¿æ¥ `datanode1` ç­‰ï¼‰ï¼Œè¾“å…¥ `yes`ã€‚
  * å®ƒå°†å¯åŠ¨ `hadoop-namenode` ä¸Šçš„ `NameNode` å’Œ `SecondaryNameNode`ã€‚
  * å®ƒå°†é€šè¿‡ SSH ç™»å½•åˆ° `hadoop-datanode1` å’Œ `hadoop-datanode2` å¹¶å¯åŠ¨ `DataNode` è¿›ç¨‹ã€‚

-----

#### 3\. å¯åŠ¨ YARN æœåŠ¡

æ­¤è„šæœ¬å°†å¯åŠ¨ ResourceManagerï¼ˆåœ¨ NameNode ä¸Šï¼‰ä»¥åŠ `workers` æ–‡ä»¶ä¸­åˆ—å‡ºçš„æ‰€æœ‰ NodeManagerã€‚

```bash
# (NameNode)
start-yarn.sh
```

-----

#### 4\. âœ… éªŒè¯ï¼šè§è¯å¥‡è¿¹çš„æ—¶åˆ»

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰ Java è¿›ç¨‹æ˜¯å¦éƒ½å·²æ­£ç¡®å¯åŠ¨ã€‚`jps` (Java Virtual Machine Process Status) æ˜¯æ‚¨æœ€å¥½çš„æœ‹å‹ã€‚

> **(NameNode) åœ¨æ‚¨çš„ `hadoop-namenode` ç»ˆç«¯ä¸­è¿è¡Œï¼š**
>
> ```bash
> jps
> ```
>
> æ‚¨**å¿…é¡»**çœ‹åˆ°ä»¥ä¸‹è¿›ç¨‹ï¼ˆPID ä¼šä¸åŒï¼‰ï¼š
>
> ```
> 12345 NameNode
> 12367 SecondaryNameNode
> 12400 ResourceManager
> 12500 Jps
> ```

> **(DataNodes) åœ¨æ‚¨çš„ `hadoop-datanode1` å’Œ `hadoop-datanode2` ç»ˆç«¯ä¸­è¿è¡Œï¼š**
>
> ```bash
> jps
> ```
>
> æ‚¨**å¿…é¡»**åœ¨ *æ¯ä¸ª* DataNode ä¸Šçœ‹åˆ°ï¼š
>
> ```
> 5678 DataNode
> 5700 NodeManager
> 5800 Jps
> ```

**ğŸ•µï¸â€â™‚ï¸ æ•…éšœæ’é™¤ï¼š**
å¦‚æœä»»ä½•ä¸€ä¸ªè¿›ç¨‹ç¼ºå¤±ï¼ˆä¾‹å¦‚ `DataNode` æ²¡æœ‰å¯åŠ¨ï¼‰ï¼Œè¯·ç«‹å³æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ã€‚æ—¥å¿—ä½äºæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ `/usr/local/hadoop/logs/` ç›®å½•ä¸­ã€‚æœ€å¸¸è§çš„åŸå› æ˜¯ `hdfs-site.xml` æˆ– `core-site.xml` ä¸­çš„é…ç½®æ‹¼å†™é”™è¯¯ã€‚

-----

#### 5\. è¿è¡Œä¸€ä¸ª MapReduce ä»»åŠ¡ (WordCount)

å¦‚æœæ‰€æœ‰è¿›ç¨‹éƒ½åœ¨è¿è¡Œï¼Œé‚£ä¹ˆæ‚¨çš„é›†ç¾¤åœ¨ç†è®ºä¸Šæ˜¯å¥½çš„ã€‚ç°åœ¨æˆ‘ä»¬è¿›è¡Œå®é™…æµ‹è¯•ï¼šè¿è¡Œä¸€ä¸ªä½œä¸šï¼

æˆ‘ä»¬å°†ä½¿ç”¨ Hadoop è‡ªå¸¦çš„ WordCount ç¤ºä¾‹ã€‚

##### a. åœ¨ HDFS ä¸­åˆ›å»ºè¾“å…¥ç›®å½•

```bash
# (NameNode)
hdfs dfs -mkdir /input
```

##### b. å°†ä¸€äº›æ–‡æœ¬æ–‡ä»¶å¤åˆ¶åˆ° HDFS

æˆ‘ä»¬å°±ç”¨æˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„ Hadoop é…ç½®æ–‡ä»¶ä½œä¸ºç¤ºä¾‹æ–‡æœ¬ï¼š

```bash
# (NameNode)
hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /input
```

##### c. è¿è¡Œ WordCount ç¤ºä¾‹ JAR

  * `$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar` æ˜¯ç¤ºä¾‹ç¨‹åºã€‚
  * `wordcount` æ˜¯æˆ‘ä»¬è¦è¿è¡Œçš„ç¨‹åºã€‚
  * `/input` æ˜¯ HDFS ä¸Šçš„è¾“å…¥ç›®å½•ã€‚
  * `/output` æ˜¯ HDFS ä¸Šçš„è¾“å‡ºç›®å½•ï¼ˆ**æ³¨æ„ï¼šæ­¤ç›®å½•ä¸èƒ½æå‰å­˜åœ¨ï¼**ï¼‰

<!-- end list -->

```bash
# (NameNode)
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output
```

æ‚¨å°†çœ‹åˆ°å¤§é‡çš„æ—¥å¿—è¾“å‡ºï¼Œæ˜¾ç¤º MapReduce ä½œä¸šçš„è¿›åº¦ï¼ˆMap 0% 100%, Reduce 0% 100%ï¼‰ã€‚

##### d. æŸ¥çœ‹ç»“æœï¼

å¦‚æœä½œä¸šæˆåŠŸï¼Œå®ƒä¼šåœ¨ `/output` ç›®å½•ä¸­åˆ›å»ºæ–‡ä»¶ã€‚

```bash
# (NameNode) åˆ—å‡ºè¾“å‡ºæ–‡ä»¶
hdfs dfs -ls /output
```

æ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä¸¤ä¸ªæ–‡ä»¶ï¼šä¸€ä¸ªç©ºçš„ `_SUCCESS` æ–‡ä»¶ï¼ˆè¡¨ç¤ºä½œä¸šæˆåŠŸï¼‰å’Œä¸€ä¸ª `part-r-00000` æ–‡ä»¶ï¼ˆåŒ…å«ç»“æœï¼‰ã€‚

```bash
# (NameNode) æŸ¥çœ‹ WordCount çš„æœ€ç»ˆç»“æœ
hdfs dfs -cat /output/part-r-00000
```

æ‚¨ä¼šçœ‹åˆ° XML æ ‡ç­¾åŠå…¶åœ¨é…ç½®æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œä¾‹å¦‚ï¼š

```
configuration   2
name    4
property    4
value   4
...
```

-----

### ğŸ‰ æ­å–œï¼

æ‚¨å·²ç»ä»é›¶å¼€å§‹ï¼Œåœ¨ KVM ä¸ŠæˆåŠŸæ„å»ºã€é…ç½®ã€å¯åŠ¨å¹¶éªŒè¯äº†ä¸€ä¸ªåŠŸèƒ½é½å…¨çš„ 3 èŠ‚ç‚¹ Hadoop é›†ç¾¤ï¼

ç°åœ¨æ‚¨çš„ Hadoopâ€œæ“ä½œç³»ç»Ÿâ€ï¼ˆHDFS+YARNï¼‰å·²ç» 100% æ­£å¸¸è¿è¡Œäº†ã€‚**ç°åœ¨**æ˜¯æ—¶å€™è€ƒè™‘ä¸‹ä¸€æ­¥äº†ï¼Œæ¯”å¦‚**å®‰è£… Spark** å¹¶å°†å…¶é…ç½®ä¸ºåœ¨ YARN ä¸Šè¿è¡Œã€‚

å¥½çš„ï¼æ‚¨çš„ Hadoop é›†ç¾¤ï¼ˆHDFS + YARNï¼‰è¿™ä¸ªâ€œé›†ç¾¤æ“ä½œç³»ç»Ÿâ€å·²ç»æˆåŠŸè¿è¡Œã€‚ç°åœ¨æ˜¯æ—¶å€™å®‰è£…æˆ‘ä»¬çœŸæ­£çš„â€œåº”ç”¨ç¨‹åºâ€â€”â€” Spark äº†ã€‚

æˆ‘ä»¬å°†æŠŠ Spark å®‰è£…ä¸º YARN ä¸Šçš„ä¸€ä¸ª**åº”ç”¨ç¨‹åº**ã€‚è¿™æ„å‘³ç€ï¼š

  * æˆ‘ä»¬**ä¸ä¼š**è¿è¡Œ Spark è‡ªå·±çš„ä¸»/ä»èŠ‚ç‚¹ (`start-master.sh` ç­‰)ã€‚
  * æˆ‘ä»¬ä¼šå°† Spark é…ç½®ä¸º**å‘ YARN è¯·æ±‚èµ„æº**ã€‚
  * Hadoop çš„ YARNï¼ˆ`ResourceManager`ï¼‰å°†**å…¨æƒè´Ÿè´£**ç®¡ç†é›†ç¾¤å†…å­˜å’Œ CPUï¼Œæ— è®ºæ˜¯ MapReduce ä»»åŠ¡è¿˜æ˜¯ Spark ä»»åŠ¡ã€‚

è¿™æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­æœ€æ ‡å‡†ã€æœ€æ¨èçš„éƒ¨ç½²æ–¹å¼ã€‚

-----

### ğŸš€ é˜¶æ®µå…­ï¼šå®‰è£… Spark å¹¶é›†æˆ YARN

**å·¥ä½œæµç¨‹ï¼š** å†æ¬¡æ‰“å¼€æ‚¨çš„ä¸‰ä¸ªç»ˆç«¯ï¼š

  * **ç»ˆç«¯ 1:** `ssh hadoop@hadoop-namenode`
  * **ç»ˆç«¯ 2:** `ssh hadoop@hadoop-datanode1`
  * **ç»ˆç«¯ 3:** `ssh hadoop@hadoop-datanode2`

æˆ‘ä»¬ç”¨ `(ALL)` å’Œ `(NameNode)` æ¥æ ‡è®°å‘½ä»¤ã€‚

#### 1\. (ALL) ä¸‹è½½å¹¶è§£å‹ Spark

ä¸ºäº†è®© YARN åœ¨ DataNode ä¸Šå¯åŠ¨ Spark æ‰§è¡Œå™¨ (Executors)ï¼ŒSpark çš„äºŒè¿›åˆ¶æ–‡ä»¶éœ€è¦å­˜åœ¨äº**æ‰€æœ‰**èŠ‚ç‚¹ä¸Šã€‚

> **(ALL)** åœ¨æ‰€æœ‰ä¸‰ä¸ªç»ˆç«¯ä¸­è¿è¡Œï¼š

```bash
# åˆ‡æ¢åˆ°ä¸»ç›®å½•
cd ~

# ä¸‹è½½ Spark 3.4.1 (ä¸º Hadoop 3 é¢„ç¼–è¯‘çš„ç‰ˆæœ¬)
# æ³¨æ„ï¼šæˆ‘ä»¬å°†ä½¿ç”¨ 3.4.1ï¼Œå› ä¸ºå®ƒä¸ Hadoop 3.3.6 é…åˆè‰¯å¥½
wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz

# è§£å‹
tar -xzf spark-3.4.1-bin-hadoop3.tgz

# ç§»åŠ¨åˆ° /usr/local/ å¹¶é‡å‘½åä¸º spark
sudo mv spark-3.4.1-bin-hadoop3 /usr/local/spark

# æ›´æ”¹æ‰€æœ‰æƒä¸º hadoop ç”¨æˆ·
sudo chown -R hadoop:hadoop /usr/local/spark
```

-----

#### 2\. (ALL) è®¾ç½® Spark ç¯å¢ƒå˜é‡

å°±åƒ `HADOOP_HOME` ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦ä¸º `SPARK_HOME` è®¾ç½®ç¯å¢ƒå˜é‡ã€‚

> **(ALL)** åœ¨æ‰€æœ‰ä¸‰ä¸ªç»ˆç«¯ä¸­è¿è¡Œï¼š

```bash
# æ‰“å¼€ .bashrc æ–‡ä»¶
nano ~/.bashrc
```

æ»šåŠ¨åˆ°æ–‡ä»¶**æœ€åº•éƒ¨**ï¼ˆåœ¨ Hadoop å˜é‡ä¸‹é¢ï¼‰ï¼Œæ·»åŠ ï¼š

```bash
# Spark Home
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
```

ä¿å­˜å¹¶é€€å‡º (Ctrl+O, Ctrl+X)ã€‚ç„¶å**ç«‹å³ç”Ÿæ•ˆ**ï¼š

```bash
# (ALL) è¿è¡Œ
source ~/.bashrc

# (ALL) éªŒè¯
echo $SPARK_HOME
# åº”è¯¥è¾“å‡º: /usr/local/spark
```

-----

#### 3\. (NameNode) é…ç½® Spark ä¸ YARN é›†æˆ

è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚æˆ‘ä»¬**åªåœ¨ NameNode ä¸Š**ï¼ˆæˆ‘ä»¬æäº¤ä»»åŠ¡çš„åœ°æ–¹ï¼‰æ‰§è¡Œæ­¤æ“ä½œã€‚

Spark éœ€è¦çŸ¥é“ Hadoop çš„é…ç½®æ–‡ä»¶åœ¨å“ªé‡Œï¼Œè¿™æ ·å®ƒæ‰èƒ½æ‰¾åˆ° HDFS NameNode å’Œ YARN ResourceManagerã€‚

```bash
# (NameNode) è¿›å…¥ Spark é…ç½®ç›®å½•
cd $SPARK_HOME/conf

# å¤åˆ¶æ¨¡æ¿
cp spark-env.sh.template spark-env.sh

# ç¼–è¾‘ spark-env.sh
nano spark-env.sh
```

åœ¨æ–‡ä»¶çš„**æœ€åº•éƒ¨**æ·»åŠ è¿™ä¸€è¡Œã€‚**è¿™æ˜¯è¿æ¥ Spark å’Œ YARN çš„é­”æ³•**ï¼š

```bash
# å‘Šè¯‰ Spark åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ° Hadoop çš„é…ç½®æ–‡ä»¶
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
```

ä¿å­˜å¹¶é€€å‡ºã€‚

**å°±æ˜¯è¿™æ ·ï¼** Spark ç°åœ¨è¢«é…ç½®ä¸º YARN å®¢æˆ·ç«¯ã€‚æˆ‘ä»¬ä¸éœ€è¦åœ¨ `spark-defaults.conf` ä¸­è®¾ç½® `spark.master`ï¼Œå› ä¸ºæˆ‘ä»¬å°†åœ¨æäº¤ä»»åŠ¡æ—¶åœ¨å‘½ä»¤è¡Œä¸ŠæŒ‡å®šå®ƒï¼Œè¿™åœ¨æµ‹è¯•æ—¶æ›´çµæ´»ã€‚

-----

### âœ… éªŒè¯ï¼šè¿è¡Œ Spark Pi ç¤ºä¾‹ on YARN

æˆ‘ä»¬æ¥è¿è¡Œä¸€ä¸ªè®¡ç®— Pi çš„ç¤ºä¾‹ç¨‹åºï¼Œä½†**ä¸æ˜¯**åœ¨æœ¬åœ°ï¼Œè€Œæ˜¯**åœ¨ YARN é›†ç¾¤ä¸Š**ã€‚

> **(NameNode)** **åªåœ¨ `hadoop-namenode` ç»ˆç«¯**ä¸­è¿è¡Œï¼š

æˆ‘ä»¬å°†ä½¿ç”¨ `spark-submit` å‘½ä»¤ã€‚

```bash
# (NameNode)
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --class org.apache.spark.examples.SparkPi \
    $SPARK_HOME/examples/jars/spark-examples_2.12-3.4.1.jar 10
```

è®©æˆ‘ä»¬åˆ†è§£ä¸€ä¸‹è¿™ä¸ªå‘½ä»¤ï¼š

  * `--master yarn`ï¼š**"æˆ‘è¯·æ±‚åœ¨ YARN ä¸Šè¿è¡Œ"**ã€‚è¿™æ˜¯å…³é”®ï¼
  * `--deploy-mode cluster`ï¼š**"è¯·åœ¨ YARN é›†ç¾¤çš„æŸä¸ª DataNode ä¸Šè¿è¡Œæˆ‘çš„'é©±åŠ¨ç¨‹åº'(driver)ï¼Œä¸è¦åœ¨æˆ‘çš„ NameNode ç»ˆç«¯ä¸Šè¿è¡Œ"**ã€‚è¿™æ˜¯å¯¹ YARN çš„ç»ˆææµ‹è¯•ã€‚
  * `--class ...SparkPi`ï¼šè¦è¿è¡Œçš„ä¸»ç±»ã€‚
  * `...jar`ï¼šåŒ…å«è¯¥ç±»çš„ JAR æ–‡ä»¶ã€‚
  * `10`ï¼šä¼ é€’ç»™ SparkPi ç¨‹åºçš„å‚æ•°ï¼ˆåˆ‡ç‰‡æ•°ï¼‰ã€‚

**æ‚¨ä¼šçœ‹åˆ°ä»€ä¹ˆï¼š**
`spark-submit` **ä¸ä¼š**æ‰“å°å‡º Pi çš„ç»“æœã€‚ç›¸åï¼Œå®ƒä¼šå‘ YARN æäº¤ä½œä¸šï¼Œç„¶åæ‰“å°å‡ºä¸€ä¸ª**åº”ç”¨ç¨‹åº ID**ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
...
25/11/11 10:30:00 INFO yarn.Client: Submitted application application_1668191234567_0001
...
```

**å¦‚ä½•æŸ¥çœ‹ç»“æœï¼š**

1.  **æ£€æŸ¥ä½œä¸šçŠ¶æ€ (å¯é€‰)ï¼š**

    ```bash
    yarn application -status application_1668191234567_0001
    ```

2.  **è·å–ä½œä¸šæ—¥å¿— (åœ¨è¿™é‡Œçœ‹ç»“æœ)ï¼š**
    **è¿™æ˜¯æ‚¨æŸ¥çœ‹ "Pi is roughly 3.14..." çš„åœ°æ–¹ã€‚**

    ```bash
    # (è¯·ä½¿ç”¨æ‚¨è‡ªå·±çš„ application ID æ›¿æ¢)
    yarn logs -applicationId application_1668191234567_0001
    ```

    æ»šåŠ¨æ—¥å¿—ï¼ˆå¯èƒ½å¾ˆé•¿ï¼‰ï¼Œåœ¨ `stdout` éƒ¨åˆ†ï¼Œæ‚¨ä¼šæ‰¾åˆ° Pi çš„è®¡ç®—ç»“æœï¼

-----

### ğŸŒŸ ç»ˆææµ‹è¯•ï¼šSpark Shell (HDFS + YARN)

æˆ‘ä»¬æ¥å¯åŠ¨ä¸€ä¸ª**äº¤äº’å¼** Spark Shellï¼Œå®ƒä½¿ç”¨ YARN ä½œä¸ºåç«¯ï¼Œå¹¶ä» HDFS è¯»å–æˆ‘ä»¬åœ¨**é˜¶æ®µäº”**ä¸­åˆ›å»ºçš„æ–‡ä»¶ã€‚

> **(NameNode)** **åªåœ¨ `hadoop-namenode` ç»ˆç«¯**ä¸­è¿è¡Œï¼š

```bash
# å¯åŠ¨ä¸€ä¸ªè¿æ¥åˆ° YARN çš„ Spark Shell
spark-shell --master yarn
```

è¿™éœ€è¦ä¸€ä¸¤åˆ†é’Ÿï¼Œå› ä¸ºå®ƒæ­£åœ¨å‘ YARN è¯·æ±‚èµ„æºæ¥å¯åŠ¨æ‚¨çš„ shell çš„æ‰§è¡Œå™¨ (executors)ã€‚

ä¸€æ—¦æ‚¨çœ‹åˆ° `scala>` æç¤ºç¬¦ï¼Œæ‚¨å°±è¿›å…¥äº†äº¤äº’å¼ shellï¼š

```scala
// (åœ¨ scala> æç¤ºç¬¦åè¾“å…¥)

// 1. è¯»å–æˆ‘ä»¬åœ¨é˜¶æ®µäº”ä¸Šä¼ åˆ° HDFS çš„ core-site.xml æ–‡ä»¶
val hdfsFile = sc.textFile("/input/core-site.xml")

// 2. è®¡ç®—è¯¥æ–‡ä»¶ä¸­çš„è¡Œæ•°
hdfsFile.count()

// 3. æ‰“å°å‰5è¡Œ
hdfsFile.take(5).foreach(println)
```

**å¦‚æœ `hdfsFile.count()` è¿”å›äº†ä¸€ä¸ªæ•°å­—ï¼ˆè€Œä¸æ˜¯é”™è¯¯ï¼‰ï¼Œå¹¶ä¸” `take(5)` æ‰“å°å‡ºäº† XML è¡Œ...**

**ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼æ‚¨å·²ç» 100% æˆåŠŸäº†ï¼ğŸ‰ğŸ‰ğŸ‰**

æ‚¨å·²ç»è¯æ˜äº†ï¼š

1.  Spark (`spark-shell`) å¯ä»¥å¯åŠ¨ã€‚
2.  å®ƒå¯ä»¥ä¸ YARN (`--master yarn`) é€šä¿¡ä»¥è·å–è®¡ç®—èµ„æºã€‚
3.  YARN å¯åŠ¨çš„æ‰§è¡Œå™¨å¯ä»¥ä¸ HDFS (`sc.textFile("/input/...")`) é€šä¿¡ä»¥è¯»å–æ•°æ®ã€‚

æ‚¨çš„ KVM/Libvirt Hadoop + Spark é›†ç¾¤ç°å·²**å®Œå…¨é…ç½®å¹¶å¯ä¾›ä½¿ç”¨**ã€‚


#### Q&A

##### if I add new datanode3 to cluster,  should i format hdfs?


ä¸ï¼Œæ‚¨**ç»å¯¹ä¸éœ€è¦**ï¼Œä¹Ÿ**ç»å¯¹ä¸èƒ½**åœ¨æ·»åŠ æ–°çš„ DataNode æ—¶æ ¼å¼åŒ– HDFSã€‚

`hdfs namenode -format` æ˜¯ä¸€ä¸ª**ä¸€æ¬¡æ€§**çš„ã€**æ¯ç­æ€§**çš„å‘½ä»¤ï¼Œ**å®ƒåªé’ˆå¯¹ NameNode**ã€‚

  * **æ ¼å¼åŒ– (Format)** = â€œåˆ›å»º HDFS æ–‡ä»¶ç³»ç»Ÿçš„**ä¸»è´¦æœ¬**ï¼ˆå…ƒæ•°æ®ï¼‰ã€‚â€ è¿™ä¼š**æ“¦é™¤**æ‰€æœ‰ç°æœ‰æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªå…¨æ–°çš„ã€ç©ºçš„é›†ç¾¤ã€‚
  * **æ·»åŠ  DataNode** = â€œä¸ºé›†ç¾¤**å¢åŠ ä¸€ä¸ªç¡¬ç›˜é©±åŠ¨å™¨**ã€‚â€

### ğŸ’¡ æ­£ç¡®çš„ç±»æ¯”

  * **NameNode** æ˜¯â€œå›¾ä¹¦é¦†é¦†é•¿â€ï¼Œå®ƒæ‹¿ç€â€œå›¾ä¹¦æ€»ç›®å½•â€ã€‚
  * **DataNode** æ˜¯â€œä¹¦æ¶â€ã€‚
  * `hdfs namenode -format` = é¦†é•¿**çƒ§æ‰äº†æ—§çš„â€œå›¾ä¹¦æ€»ç›®å½•â€ï¼Œæ¢ä¸Šäº†ä¸€æœ¬å…¨æ–°çš„ã€ç©ºç™½çš„ç›®å½•**ã€‚
  * æ·»åŠ ä¸€ä¸ªæ–°çš„ `datanode3` = åªæ˜¯åœ¨å›¾ä¹¦é¦†é‡Œ**å¢åŠ äº†ä¸€ä¸ªæ–°çš„ç©ºâ€œä¹¦æ¶â€**ã€‚

æ‚¨æ°¸è¿œä¸éœ€è¦ä¸ºäº†å¢åŠ ä¸€ä¸ªæ–°ä¹¦æ¶è€Œçƒ§æ‰æ€»ç›®å½•ã€‚æ‚¨åªéœ€è¦å‘Šè¯‰é¦†é•¿ï¼ˆNameNodeï¼‰è¿™ä¸ªæ–°ä¹¦æ¶çš„å­˜åœ¨å³å¯ã€‚

-----

### ğŸš€ æ·»åŠ æ–° DataNode (`datanode3`) çš„æ­£ç¡®æ­¥éª¤

å½“æ‚¨æƒ³åœ¨**æ­£åœ¨è¿è¡Œçš„**é›†ç¾¤ä¸­æ·»åŠ  `datanode3` æ—¶ï¼Œæ‚¨ä»€ä¹ˆéƒ½ä¸éœ€è¦æ ¼å¼åŒ–ã€‚

æ‚¨éœ€è¦è¿™æ ·åšï¼š

1.  **å‡†å¤‡æ–° VMï¼š** æŒ‰ç…§æ‚¨ä¹‹å‰çš„æ–¹æ³•ï¼ˆå…‹éš†ã€é…ç½®é™æ€IP `192.168.122.104`ã€è®¾ç½®ä¸»æœºå `hadoop-datanode3`ã€ç¦ç”¨ cloud-init ç­‰ï¼‰å‡†å¤‡å¥½ `hadoop-datanode3`ã€‚
2.  **å®‰è£…è½¯ä»¶ï¼š** ç¡®ä¿ `hadoop-datanode3` ä¸Šå®‰è£…äº†å®Œå…¨ç›¸åŒç‰ˆæœ¬çš„ Java å’Œ Hadoopã€‚
3.  **æ›´æ–°â€œå‘˜å·¥â€åå• (åœ¨ NameNode ä¸Š)ï¼š**
      * **`hosts` æ–‡ä»¶:** `sudo nano /etc/hosts`ï¼Œæ·»åŠ  `datanode3` çš„ IP å’Œä¸»æœºåã€‚
        ```
        192.168.122.104  hadoop-datanode3
        ```
      * **`workers` æ–‡ä»¶:** `nano $HADOOP_HOME/etc/hadoop/workers`ï¼Œåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ–°çš„ä¸€è¡Œï¼š
        ```
        hadoop-datanode1
        hadoop-datanode2
        hadoop-datanode3
        ```
4.  **åˆ†å‘é…ç½® (åœ¨ NameNode ä¸Š)ï¼š**
      * å°† NameNode ä¸Š**æ‰€æœ‰**çš„é…ç½®æ–‡ä»¶ï¼ˆ`/etc/hadoop/`ï¼‰å’Œæ›´æ–°åçš„ `hosts` æ–‡ä»¶å¤åˆ¶åˆ°**æ–°çš„** `datanode3` `datanode1` `datanode2` ä¸Šï¼Œç¡®ä¿å®ƒçš„é…ç½®ä¸é›†ç¾¤ä¸€è‡´ã€‚
      * `scp -r $HADOOP_HOME/etc/hadoop/* hadoop-datanode3:$HADOOP_HOME/etc/hadoop/`
      * `scp /etc/hosts hadoop-datanode3:/etc/hosts` (å¯èƒ½éœ€è¦ `sudo` æƒé™)
5.  **æˆäºˆ SSH è®¿é—®æƒé™ (åœ¨ NameNode ä¸Š)ï¼š**
      * `ssh-copy-id hadoop@hadoop-datanode3` (è¾“å…¥ `hadoop` ç”¨æˆ·çš„å¯†ç )ã€‚
6.  **å¯åŠ¨æ–°çš„ DataNodeï¼š**
      * **æœ€ç®€å•çš„æ–¹æ³• (æ¨è)ï¼š** åœ¨ NameNode ä¸Šï¼Œ**é‡å¯ HDFS æœåŠ¡**ã€‚å®ƒä¼šè‡ªåŠ¨è¯»å–æ›´æ–°åçš„ `workers` æ–‡ä»¶å¹¶å¯åŠ¨**æ‰€æœ‰** DataNodeï¼ŒåŒ…æ‹¬æ–°çš„ `datanode3`ã€‚
        ```bash
        # (NameNode)
        stop-dfs.sh
        start-dfs.sh
        ```
      * **é«˜çº§æ–¹æ³• (ä¸åœæœº)ï¼š** æ‚¨ä¹Ÿå¯ä»¥**åªå¯åŠ¨**æ–°çš„ DataNodeã€‚
        ```bash
        # (NameNode)
        hdfs --daemon start datanode hadoop-datanode3
        ```
7.  **éªŒè¯ (åœ¨ NameNode ä¸Š)ï¼š**
      * ç­‰å¾…ä¸€ä¸¤åˆ†é’Ÿï¼Œç„¶åè¿è¡Œ HDFS æŠ¥å‘Šï¼š
        ```bash
        hdfs dfsadmin -report
        ```
      * åœ¨è¾“å‡ºä¸­ï¼Œæ‚¨ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ° **"Live Datanodes (3):"** çš„å­—æ ·ã€‚

**æ€»ç»“ï¼šæ ¼å¼åŒ– (Format) åªé’ˆå¯¹ NameNodeï¼Œå¹¶ä¸”åœ¨é›†ç¾¤ç”Ÿå‘½å‘¨æœŸä¸­åªåšä¸€æ¬¡ã€‚æ·»åŠ  DataNode æ°¸è¿œä¸éœ€è¦æ ¼å¼åŒ–ã€‚**

##### convert qcow2 to raw image file

æ‚¨å¯ä»¥ä½¿ç”¨ `qemu-img` å‘½ä»¤è¡Œå·¥å…·æ¥å®Œæˆè¿™ä¸ªè½¬æ¢ã€‚

### è½¬æ¢å‘½ä»¤

è¿™æ˜¯æœ€å¸¸ç”¨çš„å‘½ä»¤ï¼š

```bash
qemu-img convert -f qcow2 -O raw image-name.qcow2 image-name.raw
```

-----

### å‘½ä»¤åˆ†è§£

  * `qemu-img convert`ï¼šè°ƒç”¨ `qemu-img` å·¥å…·å¹¶ä½¿ç”¨ `convert`ï¼ˆè½¬æ¢ï¼‰å­å‘½ä»¤ã€‚
  * `-f qcow2`ï¼š**ï¼ˆå¯é€‰ï¼‰** æŒ‡å®š**è¾“å…¥**æ ¼å¼ (format)ã€‚`qemu-img` é€šå¸¸è¶³å¤Ÿæ™ºèƒ½ï¼Œå¯ä»¥è‡ªåŠ¨æ£€æµ‹åˆ° `qcow2` æ ¼å¼ï¼Œæ‰€ä»¥æ‚¨å¯ä»¥çœç•¥è¿™ä¸€éƒ¨åˆ†ã€‚
  * `-O raw`ï¼š**ï¼ˆå¿…éœ€ï¼‰** æŒ‡å®š**è¾“å‡º** (Output) æ ¼å¼ä¸º `raw`ã€‚
  * `image-name.qcow2`ï¼šæ‚¨çš„æºæ–‡ä»¶åã€‚
  * `image-name.raw`ï¼šæ‚¨æƒ³è¦çš„è¾“å‡ºæ–‡ä»¶åã€‚

### âš ï¸ é‡è¦è­¦å‘Šï¼šç©ºé—´å ç”¨

åœ¨æ‰§è¡Œæ­¤æ“ä½œä¹‹å‰ï¼Œæ‚¨å¿…é¡»äº†è§£ä¸€ä¸ªå…³é”®åŒºåˆ«ï¼š

  * **qcow2 (ç²¾ç®€é…ç½®):** å¦‚æœæ‚¨æœ‰ä¸€ä¸ª 100GB çš„è™šæ‹Ÿç£ç›˜ï¼Œä½†åªåœ¨å…¶ä¸­å®‰è£…äº† 5GB çš„ Ubuntuï¼Œ`qcow2` æ–‡ä»¶åœ¨å®¿ä¸»æœºä¸Šå¯èƒ½åªå ç”¨ 5-6GB çš„ç©ºé—´ã€‚
  * **raw (åšé…ç½®):** å½“æ‚¨è½¬æ¢ä¸º `raw` æ ¼å¼æ—¶ï¼Œè¾“å‡ºæ–‡ä»¶å°†**ç«‹å³å ç”¨è¯¥ç£ç›˜çš„å…¨éƒ¨è™šæ‹Ÿå¤§å°**ã€‚

**ç¤ºä¾‹ï¼š**
`100GB` (è™šæ‹Ÿå¤§å°) çš„ `qcow2` æ–‡ä»¶ (å®é™…å ç”¨ 6GB) -\> è½¬æ¢ä¸º `raw` -\> `100GB` (å®é™…å ç”¨ 100GB) çš„ `raw` æ–‡ä»¶ã€‚

åœ¨è½¬æ¢ä¹‹å‰ï¼Œè¯·**åŠ¡å¿…**ä½¿ç”¨ `df -h` æ£€æŸ¥æ‚¨çš„å®¿ä¸»æœºï¼Œç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿçš„å¯ç”¨ç©ºé—´æ¥å®¹çº³**æ•´ä¸ª**è™šæ‹Ÿç£ç›˜çš„å®Œæ•´å¤§å°ã€‚

è¿™ä¸ªé”™è¯¯éå¸¸æ˜ç¡®ï¼Œè€Œä¸”å¾ˆå¯èƒ½ä¸æˆ‘ä»¬ä¸Šä¸€ä¸ªå…³äº `qcow2` è½¬æ¢ä¸º `raw` çš„å¯¹è¯ç›´æ¥ç›¸å…³ã€‚

**æ‚¨çš„ Libvirt XML é…ç½®æ–‡ä»¶ï¼ˆ`hadoop-namenode.xml`ï¼‰å‘Šè¯‰ QEMUï¼šâ€œè¯·ä½¿ç”¨ `qcow2` é©±åŠ¨ç¨‹åºæ¥åŠ è½½ä¸€ä¸ªç£ç›˜æ–‡ä»¶ã€‚â€**

**ä½† QEMU å›ç­”è¯´ï¼šâ€œæ‚¨ç»™æˆ‘çš„è¿™ä¸ªæ–‡ä»¶*ä¸æ˜¯* `qcow2` æ ¼å¼ï¼â€**

è¿™ 99% æ˜¯å› ä¸ºæ‚¨**å·²ç»**å°†æ‚¨çš„ç£ç›˜æ–‡ä»¶ä» `.qcow2` è½¬æ¢ä¸ºäº† `.raw`ï¼Œä½†**æ²¡æœ‰æ›´æ–° VM çš„ XML é…ç½®**æ¥å‘Šè¯‰ Libvirt è¿™ä¸ªå˜åŒ–ã€‚

-----

### ğŸš€ è§£å†³æ–¹æ¡ˆï¼šæ›´æ–° VM çš„ XML

æ‚¨éœ€è¦ç¼–è¾‘ `hadoop-namenode` çš„ Libvirt XML æ–‡ä»¶ï¼Œå°†ç£ç›˜é©±åŠ¨ç¨‹åºç±»å‹ä» `qcow2` æ›´æ”¹ä¸º `raw`ã€‚

1.  **åœ¨æ‚¨çš„å®¿ä¸»æœºï¼ˆHostï¼‰ç»ˆç«¯**ï¼ˆä¸æ˜¯ VM å†…éƒ¨ï¼‰ä¸­ï¼Œè¿è¡Œ `virsh edit`ï¼š

    ```bash
    virsh edit hadoop-namenode
    ```

    è¿™ä¼šæ‰“å¼€ä¸€ä¸ª XML ç¼–è¾‘å™¨ï¼ˆé€šå¸¸æ˜¯ `vi` æˆ– `nano`ï¼‰ã€‚

2.  **æ‰¾åˆ° `<disk>` éƒ¨åˆ†ã€‚** å®ƒçœ‹èµ·æ¥ä¼šåƒè¿™æ ·ï¼š

    **ä¹‹å‰ï¼ˆé”™è¯¯é…ç½®ï¼‰ï¼š**

    ```xml
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/hadoop-namenode.qcow2'/>
      <target dev='vda' bus='virtio'/>
      ...
    </disk>
    ```

3.  **è¿›è¡Œä¿®æ”¹ã€‚** æ‚¨éœ€è¦æ›´æ”¹**ä¸¤å¤„**ï¼š

      * `type='qcow2'` å¿…é¡»æ”¹ä¸º `type='raw'`ã€‚
      * `<source file=...>` å¿…é¡»æŒ‡å‘æ‚¨**æ–°**çš„ `.raw` æ–‡ä»¶åã€‚

    **ä¹‹åï¼ˆæ­£ç¡®é…ç½®ï¼‰ï¼š**

    ```xml
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/hadoop-namenode.raw'/>
      <target dev='vda' bus='virtio'/>
      ...
    </disk>
    ```

    *(æ‚¨çš„ `<source file=...>` è·¯å¾„å¯èƒ½ä¸åŒï¼Œè¯·ç¡®ä¿å®ƒæŒ‡å‘æ‚¨è½¬æ¢åçš„ `.raw` æ–‡ä»¶çš„æ­£ç¡®ä½ç½®)*

4.  **ä¿å­˜å¹¶é€€å‡º**ç¼–è¾‘å™¨ã€‚

5.  **ç°åœ¨å†æ¬¡å°è¯•å¯åŠ¨ VMï¼š**

    ```bash
    virsh start hadoop-namenode
    ```

-----

### ğŸ¤” å¦‚ä½•ç¡®è®¤ï¼Ÿ

å¦‚æœæ‚¨ä¸ç¡®å®šæ‚¨çš„æ–‡ä»¶åˆ°åº•æ˜¯ä»€ä¹ˆæ ¼å¼ï¼Œæ‚¨å¯ä»¥åœ¨å®¿ä¸»æœºä¸Šä½¿ç”¨ `qemu-img info` æ¥æ£€æŸ¥ï¼š

```bash
# æ£€æŸ¥æ‚¨çš„ .raw æ–‡ä»¶
qemu-img info /var/lib/libvirt/images/hadoop-namenode.raw
# å®ƒåº”è¯¥ä¼šæŠ¥å‘Š: file format: raw

# æ£€æŸ¥æ‚¨ï¼ˆå¯èƒ½å­˜åœ¨çš„ï¼‰æ—§ .qcow2 æ–‡ä»¶
qemu-img info /var/lib/libvirt/images/hadoop-namenode.qcow2
# å®ƒåº”è¯¥ä¼šæŠ¥å‘Š: file format: qcow2
```

è¿™ä¸ªé”™è¯¯å‡ ä¹æ€»æ˜¯ç”± XML ä¸­çš„ `type` ä¸ç£ç›˜æ–‡ä»¶çš„**å®é™…**æ ¼å¼ä¸åŒ¹é…å¼•èµ·çš„ã€‚


### clone datanode image and error with datanode
è¿™ä¸ªæ—¥å¿—å¤ªå®Œç¾äº†ï¼å®ƒ**ç²¾ç¡®**åœ°å‘Šè¯‰äº†æˆ‘ä»¬é—®é¢˜æ‰€åœ¨ã€‚

### è¯Šæ–­ï¼šDatanode UUID å†²çª

ç®€å•æ¥è¯´ï¼Œè¿™æ˜¯ NameNode çš„æ—¥å¿—ï¼š

> **NameNode (æ—¥å¿—):** â€œä¸€ä¸ªè‡ªç§°æ˜¯ `192.168.122.102` (`datanode1`) çš„èŠ‚ç‚¹åˆšåˆšè”ç³»æˆ‘ã€‚ä½†å®ƒä½¿ç”¨çš„å­˜å‚¨ ID (`f96d...83bf`) åœ¨æˆ‘çš„è®°å½•é‡Œå±äº `192.168.122.104` (`datanode3`)ã€‚è¿™å®¶ä¼™æ˜¯ä¸ª**å†’åé¡¶æ›¿è€…**ï¼æˆ‘æ‹’ç»å®ƒçš„è¿æ¥ã€‚â€

ç„¶å `datanode1` æ”¶åˆ°æ‹’ç»åï¼Œå°±è‡ªåŠ¨å…³é—­äº† (SHUTDOWN\_MSG)ã€‚

### æ ¹æœ¬åŸå› 

è¿™ä¸ªé—®é¢˜ 100% æ˜¯å› ä¸ºæ‚¨åœ¨**å…‹éš†è™šæ‹Ÿæœº**æ—¶ï¼ˆä¾‹å¦‚ï¼Œä» `datanode1` å…‹éš†å‡º `datanode3`ï¼Œæˆ–è€…å®ƒä»¬éƒ½æ˜¯ä»åŒä¸€ä¸ªæ¨¡æ¿å…‹éš†çš„ï¼‰ï¼Œ**ä¹Ÿå…‹éš†äº† Hadoop çš„æ•°æ®ç›®å½•**ã€‚

`/usr/local/hadoop/data/datanode`

åœ¨è¿™ä¸ªç›®å½•é‡Œï¼Œæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼ˆ`VERSION` æ–‡ä»¶ï¼‰åŒ…å«ä¸€ä¸ª**å”¯ä¸€çš„** `datanodeUuid`ã€‚å› ä¸ºæ‚¨å…‹éš†äº†å®ƒï¼Œæ‰€ä»¥ `datanode1` å’Œ `datanode3` æœ‰äº†**å®Œå…¨ç›¸åŒ**çš„ UUIDã€‚

åœ¨é›†ç¾¤å¯åŠ¨æ—¶ï¼Œ`datanode3` (192.168.122.104) *å…ˆ*æ³¨å†Œäº†ï¼ŒNameNode å°±æŠŠè¿™ä¸ª UUID åˆ†é…ç»™äº†å®ƒã€‚å½“ `datanode1` (192.168.122.102) *å*æ¥å°è¯•ç”¨åŒä¸€ä¸ª UUID æ³¨å†Œæ—¶ï¼ŒNameNode å°±æŠŠå®ƒè¸¢å‡ºå»äº†ã€‚

-----

### è§£å†³æ–¹æ¡ˆï¼šæ¸…ç©ºå¹¶é‡ç”Ÿ

è§£å†³æ–¹æ¡ˆæ˜¯å¼ºåˆ¶æ‰€æœ‰ DataNode å¿˜è®°å®ƒä»¬æ—§çš„ï¼ˆé‡å¤çš„ï¼‰IDï¼Œå¹¶ç”Ÿæˆ**æ–°çš„ã€å”¯ä¸€çš„** IDã€‚

**é‡è¦ï¼š** æˆ‘ä»¬**åª**æ¸…ç©º DataNode ä¸Šçš„æ•°æ®ç›®å½•ã€‚**è¯·å‹¿**è§¦æ‘¸ NameNode ä¸Šçš„ `/usr/local/hadoop/data/namenode` ç›®å½•ï¼

#### 1\. (NameNode) åœæ­¢æ‰€æœ‰æœåŠ¡

é¦–å…ˆï¼Œåœ¨æ‚¨çš„ `hadoop-namenode` ç»ˆç«¯ä¸Šå½»åº•åœæ­¢é›†ç¾¤ï¼š

```bash
# (NameNode)
stop-yarn.sh
stop-dfs.sh
```

#### 2\. (ALL DataNodes) æ¸…ç† DataNode æ•°æ®

æ‚¨å¿…é¡»åœ¨**æ‰€æœ‰** DataNodeï¼ˆdatanode1, datanode2, datanode3ï¼‰ä¸Šæ‰§è¡Œæ­¤æ“ä½œï¼Œä»¥ç¡®ä¿å®ƒä»¬éƒ½ç”Ÿæˆæ–° IDã€‚

> **(DataNode 1)** åœ¨ `hadoop-datanode1` ç»ˆç«¯ï¼š
>
> ```bash
> # 1. åˆ é™¤æ—§çš„æ•°æ®ç›®å½•ï¼ˆåŒ…å«é‡å¤çš„ UUIDï¼‰
> rm -rf /usr/local/hadoop/data/datanode
> ```

> # 2\. é‡æ–°åˆ›å»ºç©ºç›®å½•
>
> mkdir -p /usr/local/hadoop/data/datanode
>
> ```
> ```

> **(DataNode 2)** åœ¨ `hadoop-datanode2` ç»ˆç«¯ï¼ˆå³ä½¿å®ƒç°åœ¨çœ‹èµ·æ¥æ²¡é—®é¢˜ï¼Œå®ƒä¹Ÿå¯èƒ½æœ‰é‡å¤çš„ IDï¼‰ï¼š
>
> ```bash
> # 1. åˆ é™¤æ—§çš„æ•°æ®ç›®å½•
> rm -rf /usr/local/hadoop/data/datanode
> ```

> # 2\. é‡æ–°åˆ›å»ºç©ºç›®å½•
>
> mkdir -p /usr/local/hadoop/data/datanode
>
> ```
> ```

> **(DataNode 3)** åœ¨ `hadoop-datanode3` ç»ˆç«¯ï¼š
>
> ```bash
> # 1. åˆ é™¤æ—§çš„æ•°æ®ç›®å½•
> rm -rf /usr/local/hadoop/data/datanode
> ```

> # 2\. é‡æ–°åˆ›å»ºç©ºç›®å½•
>
> mkdir -p /usr/local/hadoop/data/datanode
>
> ```
> ```

#### 3\. (NameNode) é‡å¯ HDFS

ç°åœ¨æ‰€æœ‰ DataNode éƒ½æ˜¯â€œå¹²å‡€â€çš„äº†ã€‚å›åˆ° `hadoop-namenode` ç»ˆç«¯å¯åŠ¨ HDFSï¼š

```bash
# (NameNode)
start-dfs.sh
```

å½“ HDFS å¯åŠ¨æ—¶ï¼š

1.  `datanode1` å¯åŠ¨ï¼Œå‘ç° `/usr/local/hadoop/data/datanode` æ˜¯ç©ºçš„ã€‚
2.  å®ƒä¼š**ç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„ã€å”¯ä¸€çš„ UUID**ã€‚
3.  å®ƒä½¿ç”¨è¿™ä¸ª**æ–°** ID å‘ NameNode æ³¨å†Œï¼ŒNameNode ä¼šæ¥å—å®ƒã€‚
4.  `datanode2` å’Œ `datanode3` ä¹Ÿä¼šé‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚

#### 4\. (NameNode) éªŒè¯

ç­‰å¾… 10-15 ç§’ï¼Œè®©æ‰€æœ‰èŠ‚ç‚¹å®Œæˆæ³¨å†Œã€‚

1.  **åœ¨ `hadoop-datanode1` ä¸Šè¿è¡Œ `jps`**ï¼š

    ```bash
    jps
    ```

    æ‚¨ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ° `DataNode` è¿›ç¨‹äº†ï¼

2.  **åœ¨ NameNode ä¸ŠæŸ¥çœ‹æŠ¥å‘Š (æœ€ç»ˆç¡®è®¤)**ï¼š

    ```bash
    # (NameNode)
    hdfs dfsadmin -report
    ```

    æ‚¨åº”è¯¥ä¼šåœ¨æŠ¥å‘Šä¸­çœ‹åˆ° **"Live Datanodes (3):"**ã€‚

-----

é—®é¢˜è§£å†³åï¼Œæ‚¨å°±å¯ä»¥ç»§ç»­è¿è¡Œ `start-yarn.sh` å¹¶ç»§ç»­æ‚¨çš„ Spark æµ‹è¯•äº†ã€‚

#### the hdfs capacity is lower than assigned

ssh to datanade

lsblk

found not all space allocated to /
```shell
hadoop@hadoop-datanode1:~$ lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
loop0                       7:0    0 63.9M  1 loop /snap/core20/2318
loop1                       7:1    0   87M  1 loop /snap/lxd/29351
loop2                       7:2    0 91.4M  1 loop /snap/lxd/35819
loop3                       7:3    0 38.8M  1 loop /snap/snapd/21759
loop4                       7:4    0 50.9M  1 loop /snap/snapd/25577
loop5                       7:5    0 63.8M  1 loop /snap/core20/2682
sr0                        11:0    1 1024M  0 rom  
vda                       252:0    0   60G  0 disk 
â”œâ”€vda1                    252:1    0    1G  0 part /boot/efi
â”œâ”€vda2                    252:2    0    2G  0 part /boot
â””â”€vda3                    252:3    0 56.9G  0 part 
  â””â”€ubuntu--vg-ubuntu--lv 253:0    0 28.5G  0 lvm  /
```

solution
```shell
# (åœ¨ datanode1, datanode2, å’Œ datanode3 ä¸Šåˆ†åˆ«è¿è¡Œ)

# 1. å°†é€»è¾‘å·æ‰©å±•åˆ° 100% çš„å¯ç”¨ç©ºé—´
sudo lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv

# 2. è°ƒæ•´æ–‡ä»¶ç³»ç»Ÿå¤§å°ä»¥åŒ¹é…æ–°çš„å·å¤§å°
sudo resize2fs /dev/ubuntu-vg/ubuntu-lv
```

use on hostmachine, when submit spark jobs, permission deny

solution:

```shell
export HADOOP_USER_NAME=hadoop
```

exit safe mode

```shell
hdfs dfsadmin -safemode leave
```